{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tannisthamaiti/AIWeekend-Project/blob/main/NLP_WordEmbeddings_CNN_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00149f9f",
      "metadata": {
        "id": "00149f9f"
      },
      "source": [
        "# Word Embeddings: Hands On\n",
        "\n",
        "In previous lecture notebooks you saw all the steps needed to train the CBOW model. This notebook will walk you through how to extract the word embedding vectors from a model.\n",
        "\n",
        "Let's dive into it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e88b368",
      "metadata": {
        "id": "3e88b368"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17188b26",
      "metadata": {
        "id": "17188b26"
      },
      "source": [
        "Before moving on, you will be provided with some variables needed for further procedures, which should be familiar by now. Also a trained CBOW model will be simulated, the corresponding weights and biases are provided:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict(words):\n",
        "    \"\"\"\n",
        "    Generate word-to-index and index-to-word dictionaries.\n",
        "\n",
        "    Args:\n",
        "        words (list of str): List of words from a tokenized corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (word2Ind, Ind2word) dictionaries.\n",
        "    \"\"\"\n",
        "    unique_words = sorted(set(words))  # Sort for consistency\n",
        "    word2Ind = {word: i for i, word in enumerate(unique_words)}\n",
        "    Ind2word = {i: word for word, i in word2Ind.items()}\n",
        "\n",
        "    return word2Ind, Ind2word\n",
        "\n",
        "# Example usage\n",
        "words = [\"hello\", \"world\", \"hello\", \"machine\", \"learning\"]\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "print(\"word2Ind:\", word2Ind)\n",
        "print(\"Ind2word:\", Ind2word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfH1ehk8swOs",
        "outputId": "78aff8b7-b33a-4bfa-df85-7036b1ce0ca9"
      },
      "id": "dfH1ehk8swOs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2Ind: {'hello': 0, 'learning': 1, 'machine': 2, 'world': 3}\n",
            "Ind2word: {0: 'hello', 1: 'learning', 2: 'machine', 3: 'world'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30b8923",
      "metadata": {
        "id": "a30b8923"
      },
      "outputs": [],
      "source": [
        "# Define the tokenized version of the corpus\n",
        "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
        "\n",
        "# Define V. Remember this is the size of the vocabulary\n",
        "V = 5\n",
        "\n",
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "\n",
        "# Define first matrix of weights\n",
        "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "\n",
        "# Define second matrix of weights\n",
        "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "               [ 0.07055222, -0.02015138,  0.36107434],\n",
        "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "\n",
        "# Define first vector of biases\n",
        "b1 = np.array([[ 0.09688219],\n",
        "               [ 0.29239497],\n",
        "               [-0.27364426]])\n",
        "\n",
        "# Define second vector of biases\n",
        "b2 = np.array([[ 0.0352008 ],\n",
        "               [-0.36393384],\n",
        "               [-0.12775555],\n",
        "               [-0.34802326],\n",
        "               [-0.07017815]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08070a0a",
      "metadata": {
        "id": "08070a0a"
      },
      "source": [
        "\n",
        "\n",
        "## Extracting word embedding vectors\n",
        "\n",
        "Once you have finished training the neural network, you have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices $\\mathbf{W_1}$ and/or $\\mathbf{W_2}$.\n",
        "\n",
        "### Option 1: extract embedding vectors from $\\mathbf{W_1}$\n",
        "\n",
        "The first option is to take the columns of $\\mathbf{W_1}$ as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n",
        "\n",
        "> Note: in this practice notebooks the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here's how you would proceed after the training process is complete.\n",
        "\n",
        "For example $\\mathbf{W_1}$ is this matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b4d24c",
      "metadata": {
        "id": "a6b4d24c"
      },
      "outputs": [],
      "source": [
        "# Print W1\n",
        "W1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "233658d6",
      "metadata": {
        "id": "233658d6"
      },
      "source": [
        "The first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\n",
        "\n",
        "The first, second, etc. words are ordered as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d313656",
      "metadata": {
        "id": "1d313656"
      },
      "outputs": [],
      "source": [
        "# Print corresponding word for each index within vocabulary's range\n",
        "for i in range(V):\n",
        "    print(Ind2word[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a71e4d43",
      "metadata": {
        "id": "a71e4d43"
      },
      "source": [
        "So the word embedding vectors corresponding to each word are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9094092",
      "metadata": {
        "id": "b9094092"
      },
      "outputs": [],
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W1[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2674192e",
      "metadata": {
        "id": "2674192e"
      },
      "source": [
        "### Option 2: extract embedding vectors from $\\mathbf{W_2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d191af4",
      "metadata": {
        "id": "4d191af4"
      },
      "source": [
        "The second option is to take $\\mathbf{W_2}$ transposed, and take its columns as the word embedding vectors just like you did for $\\mathbf{W_1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66af03b0",
      "metadata": {
        "id": "66af03b0"
      },
      "outputs": [],
      "source": [
        "# Print transposed W2\n",
        "W2.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ce56b1",
      "metadata": {
        "id": "31ce56b1"
      },
      "outputs": [],
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W2.T[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c4567eb",
      "metadata": {
        "id": "8c4567eb"
      },
      "source": [
        "### Option 3: extract embedding vectors from $\\mathbf{W_1}$ and $\\mathbf{W_2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bac0e58",
      "metadata": {
        "id": "0bac0e58"
      },
      "source": [
        "The third option, which is the one you will use in this week's assignment, uses the average of $\\mathbf{W_1}$ and $\\mathbf{W_2^\\top}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79ba388d",
      "metadata": {
        "id": "79ba388d"
      },
      "source": [
        "**Calculate the average of $\\mathbf{W_1}$ and $\\mathbf{W_2^\\top}$, and store the result in `W3`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35a13db",
      "metadata": {
        "id": "f35a13db"
      },
      "outputs": [],
      "source": [
        "# Compute W3 as the average of W1 and W2 transposed\n",
        "W3 = (W1+W2.T)/2\n",
        "\n",
        "# Print W3\n",
        "W3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a39965c",
      "metadata": {
        "id": "9a39965c"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "    array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
        "           [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
        "           [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8802bbb1",
      "metadata": {
        "id": "8802bbb1"
      },
      "source": [
        "Extracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you've just created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f32ef1f2",
      "metadata": {
        "id": "f32ef1f2"
      },
      "outputs": [],
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W3[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c76a6a4",
      "metadata": {
        "id": "7c76a6a4"
      },
      "source": [
        "Now you know 3 different options to get the word embedding vectors from a model!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a0c967b",
      "metadata": {
        "id": "5a0c967b"
      },
      "source": [
        "### How this practice relates to and differs from the upcoming graded assignment\n",
        "\n",
        "- After extracting the word embedding vectors, you will use principal component analysis (PCA) to visualize the vectors, which will enable you to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed5330c1",
      "metadata": {
        "id": "ed5330c1"
      },
      "source": [
        "# Word Embeddings: Training the CBOW model\n",
        "\n",
        "In previous lecture notebooks you saw how to prepare data before feeding it to a continuous bag-of-words model, the model itself, its architecture and activation functions. This notebook will walk you through:\n",
        "\n",
        "- Forward propagation.\n",
        "\n",
        "- Cross-entropy loss.\n",
        "\n",
        "- Backpropagation.\n",
        "\n",
        "- Gradient descent.\n",
        "\n",
        "Which are concepts necessary to understand how the training of the model works.\n",
        "\n",
        "Let's dive into it!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d956ba9",
      "metadata": {
        "id": "0d956ba9"
      },
      "source": [
        "## Forward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b11906",
      "metadata": {
        "id": "60b11906"
      },
      "source": [
        "Let's dive into the neural network itself, which is shown below with all the dimensions and formulas you'll need.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='cbow_model_dimensions_single_input.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:839;height:349;\" /> Figure 2 </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40919bc2",
      "metadata": {
        "id": "40919bc2"
      },
      "source": [
        "Set $N$ equal to 3. Remember that $N$ is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\n",
        "\n",
        "Also set $V$ equal to 5, which is the size of the vocabulary we have used so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "182ed42f",
      "metadata": {
        "id": "182ed42f"
      },
      "outputs": [],
      "source": [
        "# Define the size of the word embedding vectors and save it in the variable 'N'\n",
        "N = 3\n",
        "\n",
        "# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\n",
        "V = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59a1e099",
      "metadata": {
        "id": "59a1e099"
      },
      "source": [
        "### Initialization of the weights and biases"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caf5e06f",
      "metadata": {
        "id": "caf5e06f"
      },
      "source": [
        "Before you start training the neural network, you need to initialize the weight matrices and bias vectors with random values.\n",
        "\n",
        "In the assignment you will implement a function to do this yourself using `numpy.random.rand`. In this notebook, we've pre-populated these matrices and vectors for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "498a8330",
      "metadata": {
        "id": "498a8330"
      },
      "outputs": [],
      "source": [
        "# Define first matrix of weights\n",
        "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "\n",
        "# Define second matrix of weights\n",
        "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "               [ 0.07055222, -0.02015138,  0.36107434],\n",
        "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "\n",
        "# Define first vector of biases\n",
        "b1 = np.array([[ 0.09688219],\n",
        "               [ 0.29239497],\n",
        "               [-0.27364426]])\n",
        "\n",
        "# Define second vector of biases\n",
        "b2 = np.array([[ 0.0352008 ],\n",
        "               [-0.36393384],\n",
        "               [-0.12775555],\n",
        "               [-0.34802326],\n",
        "               [-0.07017815]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0329518b",
      "metadata": {
        "id": "0329518b"
      },
      "source": [
        "**Check that the dimensions of these matrices match those shown in the figure above.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55b2124d",
      "metadata": {
        "id": "55b2124d"
      },
      "outputs": [],
      "source": [
        "print(f'V (vocabulary size): {V}')\n",
        "print(f'N (embedding size / size of the hidden layer): {N}')\n",
        "print(f'size of W1: {W1.shape} (NxV)')\n",
        "print(f'size of b1: {b1.shape} (Nx1)')\n",
        "print(f'size of W2: {W2.shape} (VxN)')\n",
        "print(f'size of b2: {b2.shape} (Vx1)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "188c34aa",
      "metadata": {
        "id": "188c34aa"
      },
      "source": [
        "\n",
        "Before moving forward, you will need some functions and variables defined in previous notebooks. They can be found next. Be sure you understand everything that is going on in the next cell, if not consider doing a refresh of the first lecture notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b619ec97",
      "metadata": {
        "id": "b619ec97"
      },
      "outputs": [],
      "source": [
        "# Define the tokenized version of the corpus\n",
        "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
        "\n",
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "# Define the 'get_windows' function as seen in a previous notebook\n",
        "def get_windows(words, C):\n",
        "    i = C\n",
        "    while i < len(words) - C:\n",
        "        center_word = words[i]\n",
        "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
        "        yield context_words, center_word\n",
        "        i += 1\n",
        "\n",
        "# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\n",
        "def word_to_one_hot_vector(word, word2Ind, V):\n",
        "    one_hot_vector = np.zeros(V)\n",
        "    one_hot_vector[word2Ind[word]] = 1\n",
        "    return one_hot_vector\n",
        "\n",
        "# Define the 'context_words_to_vector' function as seen in a previous notebook\n",
        "def context_words_to_vector(context_words, word2Ind, V):\n",
        "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
        "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
        "    return context_words_vectors\n",
        "\n",
        "# Define the generator function 'get_training_example' as seen in a previous notebook\n",
        "def get_training_example(words, C, word2Ind, V):\n",
        "    for context_words, center_word in get_windows(words, C):\n",
        "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d57c5004",
      "metadata": {
        "id": "d57c5004"
      },
      "source": [
        "### Training example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ced3c1",
      "metadata": {
        "id": "d4ced3c1"
      },
      "source": [
        "Run the next cells to get the first training example, made of the vector representing the context words \"i am because i\", and the target which is the one-hot vector representing the center word \"happy\".\n",
        "\n",
        "> You don't need to worry about the Python syntax, but there are some explanations below if you want to know what's happening behind the scenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5d2007",
      "metadata": {
        "id": "5b5d2007"
      },
      "outputs": [],
      "source": [
        "# Save generator object in the 'training_examples' variable with the desired arguments\n",
        "training_examples = get_training_example(words, 2, word2Ind, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e8dc4c",
      "metadata": {
        "id": "12e8dc4c"
      },
      "source": [
        "> `get_training_examples`, which uses the `yield` keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that... you can iterate on (using a `for` loop for instance), to retrieve the successive values that the function generates.\n",
        ">\n",
        "> In this case `get_training_examples` `yield`s training examples, and iterating on `training_examples` will return the successive training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f1ecbe7",
      "metadata": {
        "id": "8f1ecbe7"
      },
      "outputs": [],
      "source": [
        "# Get first values from generator\n",
        "x_array, y_array = next(training_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36a4df11",
      "metadata": {
        "id": "36a4df11"
      },
      "source": [
        "> `next` is another special keyword, which gets the next available value from an iterator. Here, you'll get the very first value, which is the first training example. If you run this cell again, you'll get the next value, and so on until the iterator runs out of values to return.\n",
        ">\n",
        "> In this notebook `next` is used because you will only be performing one iteration of training. In this week's assignment with the full training over several iterations you'll use regular `for` loops with the iterator that supplies the training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a45677d7",
      "metadata": {
        "id": "a45677d7"
      },
      "source": [
        "The vector representing the context words, which will be fed into the neural network, is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52567b91",
      "metadata": {
        "id": "52567b91"
      },
      "outputs": [],
      "source": [
        "# Print context words vector\n",
        "x_array"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a87003b7",
      "metadata": {
        "id": "a87003b7"
      },
      "source": [
        "The one-hot vector representing the center word to be predicted is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a90c42",
      "metadata": {
        "id": "08a90c42"
      },
      "outputs": [],
      "source": [
        "# Print one hot vector of center word\n",
        "y_array"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560ccf26",
      "metadata": {
        "id": "560ccf26"
      },
      "source": [
        "Now convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained in a previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34568f01",
      "metadata": {
        "id": "34568f01"
      },
      "outputs": [],
      "source": [
        "# Copy vector\n",
        "x = x_array.copy()\n",
        "\n",
        "# Reshape it\n",
        "x.shape = (V, 1)\n",
        "\n",
        "# Print it\n",
        "print(f'x:\\n{x}\\n')\n",
        "\n",
        "# Copy vector\n",
        "y = y_array.copy()\n",
        "\n",
        "# Reshape it\n",
        "y.shape = (V, 1)\n",
        "\n",
        "# Print it\n",
        "print(f'y:\\n{y}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121607c2",
      "metadata": {
        "id": "121607c2"
      },
      "source": [
        "Now you will need the activation functions seen before. Again, if this feel unfamiliar consider checking the previous lecture notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03d170a",
      "metadata": {
        "id": "e03d170a"
      },
      "outputs": [],
      "source": [
        "# Define the 'relu' function as seen in the previous lecture notebook\n",
        "def relu(z):\n",
        "    #Your code here\n",
        "    pass\n",
        "\n",
        "# Define the 'softmax' function as seen in the previous lecture notebook\n",
        "def softmax(z):\n",
        "    #Your code here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "902ea9c3",
      "metadata": {
        "id": "902ea9c3"
      },
      "source": [
        "### Values of the hidden layer\n",
        "\n",
        "Now that you have initialized all the variables that you need for forward propagation, you can calculate the values of the hidden layer using the following formulas:\n",
        "\n",
        "\\begin{align}\n",
        " \\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n",
        " \\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n",
        "\\end{align}\n",
        "\n",
        "First, you can calculate the value of $\\mathbf{z_1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0afa41b9",
      "metadata": {
        "id": "0afa41b9"
      },
      "outputs": [],
      "source": [
        "# Compute z1 (values of first hidden layer before applying the ReLU function)\n",
        "z1 = #Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee4acca",
      "metadata": {
        "id": "2ee4acca"
      },
      "source": [
        ">¬†`np.dot` is numpy's function for matrix multiplication.\n",
        "\n",
        "As expected you get an $N$ by 1 matrix, or column vector with $N$ elements, where $N$ is equal to the embedding size, which is 3 in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d29fe8",
      "metadata": {
        "id": "96d29fe8"
      },
      "outputs": [],
      "source": [
        "# Print z1\n",
        "z1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3406491",
      "metadata": {
        "id": "c3406491"
      },
      "source": [
        "You can now take the ReLU of $\\mathbf{z_1}$ to get $\\mathbf{h}$, the vector with the values of the hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53fdc179",
      "metadata": {
        "id": "53fdc179"
      },
      "outputs": [],
      "source": [
        "# Compute h (z1 after applying ReLU function)\n",
        "h = relu(z1)\n",
        "\n",
        "# Print h\n",
        "h"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fdba259",
      "metadata": {
        "id": "9fdba259"
      },
      "source": [
        "Applying ReLU means that the negative element of $\\mathbf{z_1}$ has been replaced with a zero."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fea1462",
      "metadata": {
        "id": "9fea1462"
      },
      "source": [
        "### Values of the output layer\n",
        "\n",
        "Here are the formulas you need to calculate the values of the output layer, represented by the vector $\\mathbf{\\hat y}$:\n",
        "\n",
        "\\begin{align}\n",
        " \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n",
        " \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n",
        "\\end{align}\n",
        "\n",
        "**First, calculate $\\mathbf{z_2}$.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ccd28a",
      "metadata": {
        "id": "b6ccd28a"
      },
      "outputs": [],
      "source": [
        "# Compute z2 (values of the output layer before applying the softmax function)\n",
        "z2 = #Your code here\n",
        "\n",
        "# Print z2\n",
        "z2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "737aedb7",
      "metadata": {
        "id": "737aedb7"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "    array([[-0.31973737],\n",
        "           [-0.28125477],\n",
        "           [-0.09838369],\n",
        "           [-0.33512159],\n",
        "           [-0.19919612]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40782380",
      "metadata": {
        "id": "40782380"
      },
      "source": [
        "This is a $V$ by 1 matrix, where $V$ is the size of the vocabulary, which is 5 in this example."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449df105",
      "metadata": {
        "id": "449df105"
      },
      "source": [
        "**Now calculate the value of $\\mathbf{\\hat y}$.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f825b6a",
      "metadata": {
        "id": "9f825b6a"
      },
      "outputs": [],
      "source": [
        "# Compute y_hat (z2 after applying softmax function)\n",
        "y_hat = #Your code here\n",
        "\n",
        "# Print y_hat\n",
        "y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "137539e4",
      "metadata": {
        "id": "137539e4"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "    array([[0.18519074],\n",
        "           [0.19245626],\n",
        "           [0.23107446],\n",
        "           [0.18236353],\n",
        "           [0.20891502]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e62e0363",
      "metadata": {
        "id": "e62e0363"
      },
      "source": [
        "As you've performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\n",
        "\n",
        "**That being said, what word did the neural network predict?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f42f9961",
      "metadata": {
        "id": "f42f9961"
      },
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Solution</b></font>\n",
        "</summary>\n",
        "<p>The neural network predicted the word \"happy\": the largest element of $\\mathbf{\\hat y}$ is the third one, and the third word of the vocabulary is \"happy\".</p>\n",
        "<p>Here's how you could implement this in Python:</p>\n",
        "<p><code>print(Ind2word[np.argmax(y_hat)])</code></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10e99c5e",
      "metadata": {
        "id": "10e99c5e"
      },
      "source": [
        "Well done, you've completed the forward propagation phase!\n",
        "\n",
        "## Cross-entropy loss\n",
        "\n",
        "Now that you have the network's prediction, you can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n",
        "\n",
        "> Remember that you are working on a single training example, not on a batch of examples, which is why you are using *loss* and not *cost*, which is the generalized form of loss.\n",
        "\n",
        "First let's recall what the prediction was."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b371887b",
      "metadata": {
        "id": "b371887b"
      },
      "outputs": [],
      "source": [
        "# Print prediction\n",
        "y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e95cac2",
      "metadata": {
        "id": "4e95cac2"
      },
      "source": [
        "And the actual target value is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34242c90",
      "metadata": {
        "id": "34242c90"
      },
      "outputs": [],
      "source": [
        "# Print target value\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f7d89b",
      "metadata": {
        "id": "23f7d89b"
      },
      "source": [
        "The formula for cross-entropy loss is:\n",
        "\n",
        "$$ J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}$$\n",
        "\n",
        "**Try implementing the cross-entropy loss function so you get more familiar working with numpy**\n",
        "\n",
        "Here are a some hints if you're stuck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcc2d970",
      "metadata": {
        "id": "dcc2d970"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(y_predicted, y_actual):\n",
        "    # Fill the loss variable with your code\n",
        "    loss = None\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0a4c537",
      "metadata": {
        "id": "f0a4c537"
      },
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hint 1</b></font>\n",
        "</summary>\n",
        "    <p>To multiply two numpy matrices (such as <code>y</code> and <code>y_hat</code>) element-wise, you can simply use the <code>*</code> operator.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e48e297",
      "metadata": {
        "id": "8e48e297"
      },
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hint 2</b></font>\n",
        "</summary>\n",
        "<p>Once you have a vector equal to the element-wise multiplication of <code>y</code> and <code>y_hat</code>, you can use <code>np.sum</code> to calculate the sum of the elements of this vector.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e43d2f39",
      "metadata": {
        "id": "e43d2f39"
      },
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Solution</b></font>\n",
        "</summary>\n",
        "<p><code>loss = np.sum(-np.log(y_hat)*y)</code></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c48f8a",
      "metadata": {
        "id": "d0c48f8a"
      },
      "source": [
        "Don't forget to run the cell containing the `cross_entropy_loss` function once it is solved."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b49d7fb",
      "metadata": {
        "id": "5b49d7fb"
      },
      "source": [
        "**Now use this function to calculate the loss with the actual values of $\\mathbf{y}$ and $\\mathbf{\\hat y}$.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa9de42",
      "metadata": {
        "id": "eaa9de42"
      },
      "outputs": [],
      "source": [
        "# Print value of cross entropy loss for prediction and target value\n",
        "cross_entropy_loss(y_hat, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49388195",
      "metadata": {
        "id": "49388195"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "    1.4650152923611106"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43763d28",
      "metadata": {
        "id": "43763d28"
      },
      "source": [
        "This value is neither good nor bad, which is expected as the neural network hasn't learned anything yet.\n",
        "\n",
        "The actual learning will start during the next phase: backpropagation.\n",
        "\n",
        "## Backpropagation\n",
        "\n",
        "The formulas that you will implement for backpropagation are the following.\n",
        "\n",
        "\\begin{align}\n",
        " \\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n",
        " \\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n",
        " \\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n",
        " \\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n",
        "\\end{align}\n",
        "\n",
        "> Note: these formulas are slightly simplified compared to the ones in the lecture as you're working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you'll be implementing the latter.\n",
        "\n",
        "Let's start with an easy one.\n",
        "\n",
        "**Calculate the partial derivative of the loss function with respect to $\\mathbf{b_2}$, and store the result in `grad_b2`.**\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b67fd2",
      "metadata": {
        "id": "d4b67fd2"
      },
      "outputs": [],
      "source": [
        "# Compute vector with partial derivatives of loss function with respect to b2\n",
        "grad_b2 = #Your code here\n",
        "\n",
        "# Print this vector\n",
        "grad_b2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32987e0a",
      "metadata": {
        "id": "32987e0a"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "    array([[ 0.18519074],\n",
        "           [ 0.19245626],\n",
        "           [-0.76892554],\n",
        "           [ 0.18236353],\n",
        "           [ 0.20891502]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2ea621a",
      "metadata": {
        "id": "a2ea621a"
      },
      "source": [
        "**Next, calculate the partial derivative of the loss function with respect to $\\mathbf{W_2}$, and store the result in `grad_W2`.**\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}$$\n",
        "\n",
        "> Hint: use `.T` to get a transposed matrix, e.g. `h.T` returns $\\mathbf{h^\\top}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df81c0e9",
      "metadata": {
        "id": "df81c0e9"
      },
      "outputs": [],
      "source": [
        "# Compute matrix with partial derivatives of loss function with respect to W2\n",
        "grad_W2 = #Your code here\n",
        "\n",
        "# Print matrix\n",
        "grad_W2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "182e0123",
      "metadata": {
        "id": "182e0123"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "    array([[ 0.06756476,  0.11798563,  0.        ],\n",
        "           [ 0.0702155 ,  0.12261452,  0.        ],\n",
        "           [-0.28053384, -0.48988499,  0.        ],\n",
        "           [ 0.06653328,  0.1161844 ,  0.        ],\n",
        "           [ 0.07622029,  0.13310045,  0.        ]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deecaa64",
      "metadata": {
        "id": "deecaa64"
      },
      "source": [
        "**Now calculate the partial derivative with respect to $\\mathbf{b_1}$ and store the result in `grad_b1`.**\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4dd8abe",
      "metadata": {
        "id": "c4dd8abe"
      },
      "outputs": [],
      "source": [
        "# Compute vector with partial derivatives of loss function with respect to b1\n",
        "grad_b1 = relu(np.dot(W2.T, y_hat - y))\n",
        "\n",
        "# Print vector\n",
        "grad_b1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebbc2572",
      "metadata": {
        "id": "ebbc2572"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "    array([[0.        ],\n",
        "           [0.        ],\n",
        "           [0.17045858]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15476c69",
      "metadata": {
        "id": "15476c69"
      },
      "source": [
        "**Finally, calculate the partial derivative of the loss with respect to $\\mathbf{W_1}$, and store it in `grad_W1`.**\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11fa35d",
      "metadata": {
        "id": "c11fa35d"
      },
      "outputs": [],
      "source": [
        "# Compute matrix with partial derivatives of loss function with respect to W1\n",
        "grad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n",
        "\n",
        "# Print matrix\n",
        "grad_W1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d662cef3",
      "metadata": {
        "id": "d662cef3"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "    array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
        "           [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
        "           [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72b8409",
      "metadata": {
        "id": "d72b8409"
      },
      "source": [
        "Before moving on to gradient descent, double-check that all the matrices have the expected dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96da4e01",
      "metadata": {
        "id": "96da4e01"
      },
      "outputs": [],
      "source": [
        "print(f'V (vocabulary size): {V}')\n",
        "print(f'N (embedding size / size of the hidden layer): {N}')\n",
        "print(f'size of grad_W1: {grad_W1.shape} (NxV)')\n",
        "print(f'size of grad_b1: {grad_b1.shape} (Nx1)')\n",
        "print(f'size of grad_W2: {grad_W2.shape} (VxN)')\n",
        "print(f'size of grad_b2: {grad_b2.shape} (Vx1)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8a2d1e8",
      "metadata": {
        "id": "a8a2d1e8"
      },
      "source": [
        "## Gradient descent\n",
        "\n",
        "During the gradient descent phase, you will update the weights and biases by subtracting $\\alpha$ times the gradient from the original matrices and vectors, using the following formulas.\n",
        "\n",
        "\\begin{align}\n",
        " \\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n",
        " \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n",
        " \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n",
        " \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n",
        "\\end{align}\n",
        "\n",
        "First, let set a value for $\\alpha$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "101a8c1a",
      "metadata": {
        "id": "101a8c1a"
      },
      "outputs": [],
      "source": [
        "# Define alpha\n",
        "alpha = 0.03"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c5eda66",
      "metadata": {
        "id": "6c5eda66"
      },
      "source": [
        "The updated weight matrix $\\mathbf{W_1}$ will be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0338ed9a",
      "metadata": {
        "id": "0338ed9a"
      },
      "outputs": [],
      "source": [
        "# Compute updated W1\n",
        "W1_new = #Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5347425e",
      "metadata": {
        "id": "5347425e"
      },
      "source": [
        "Let's compare the previous and new values of $\\mathbf{W_1}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f7e867a",
      "metadata": {
        "id": "6f7e867a"
      },
      "outputs": [],
      "source": [
        "print('old value of W1:')\n",
        "print(W1)\n",
        "print()\n",
        "print('new value of W1:')\n",
        "print(W1_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b978e12",
      "metadata": {
        "id": "5b978e12"
      },
      "source": [
        "The difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\n",
        "\n",
        "**Now calculate the new values of $\\mathbf{W_2}$ (to be stored in `W2_new`), $\\mathbf{b_1}$ (in `b1_new`), and $\\mathbf{b_2}$ (in `b2_new`).**\n",
        "\n",
        "\\begin{align}\n",
        " \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n",
        " \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n",
        " \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d5d7c64",
      "metadata": {
        "id": "1d5d7c64"
      },
      "outputs": [],
      "source": [
        "# Compute updated W2\n",
        "W2_new = #Your code here\n",
        "\n",
        "# Compute updated b1\n",
        "b1_new = #Your code here\n",
        "\n",
        "# Compute updated b2\n",
        "b2_new = #Your code here\n",
        "\n",
        "\n",
        "print('W2_new')\n",
        "print(W2_new)\n",
        "print()\n",
        "print('b1_new')\n",
        "print(b1_new)\n",
        "print()\n",
        "print('b2_new')\n",
        "print(b2_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9989305d",
      "metadata": {
        "id": "9989305d"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "    W2_new\n",
        "    [[-0.22384758 -0.43362588  0.13310965]\n",
        "     [ 0.08265956  0.0775535   0.1772054 ]\n",
        "     [ 0.19557112 -0.04637608 -0.1790735 ]\n",
        "     [ 0.06855622 -0.02363691  0.36107434]\n",
        "     [ 0.33251813 -0.3982269  -0.43959196]]\n",
        "\n",
        "    b1_new\n",
        "    [[ 0.09688219]\n",
        "     [ 0.29239497]\n",
        "     [-0.27875802]]\n",
        "\n",
        "    b2_new\n",
        "    [[ 0.02964508]\n",
        "     [-0.36970753]\n",
        "     [-0.10468778]\n",
        "     [-0.35349417]\n",
        "     [-0.0764456 ]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "758bca18",
      "metadata": {
        "id": "758bca18"
      },
      "source": [
        "Congratulations, you have completed one iteration of training using one training example!\n",
        "\n",
        "You'll need many more iterations to fully train the neural network, and you can optimize the learning process by training on batches of examples, as described in the lecture. You will get to do this during this week's assignment.\n",
        "\n",
        "### How this practice relates to and differs from the upcoming graded assignment\n",
        "\n",
        "- In the assignment, for each iteration of training you will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and you will use cross-entropy cost instead of cross-entropy loss.\n",
        "- You will also complete several iterations of training, until you reach an acceptably low cross-entropy cost, at which point you can extract good word embeddings from the weight matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b925bcdf",
      "metadata": {
        "id": "b925bcdf"
      },
      "source": [
        "### üöÄ Task: Implement Tokenization\n",
        "\n",
        " Fill in the missing code below to tokenize the given text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e50b25",
      "metadata": {
        "id": "d3e50b25"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a171d1",
      "metadata": {
        "id": "15a171d1"
      },
      "source": [
        "### üèóÔ∏è Task: Define Word Embeddings\n",
        "\n",
        " Complete the function to generate word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4536cfba",
      "metadata": {
        "id": "4536cfba"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adad811a",
      "metadata": {
        "id": "adad811a"
      },
      "source": [
        "### üî• Task: Build the CNN Model\n",
        "\n",
        " Implement the missing layers of the CNN model for text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b298ec88",
      "metadata": {
        "id": "b298ec88"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1fc219a",
      "metadata": {
        "id": "f1fc219a"
      },
      "source": [
        "### üìä Task: Train and Evaluate the Model\n",
        "\n",
        " Complete the training loop and evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55bb758c",
      "metadata": {
        "id": "55bb758c"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement here\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}