{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tannisthamaiti/AIWeekend-Project/blob/main/Transformer_chatbot_Question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wurLjheWz0x"
      },
      "outputs": [],
      "source": [
        "!pip install datasets torch transformers\n",
        "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
        "!unzip -qq cornell_movie_dialogs_corpus.zip\n",
        "!rm cornell_movie_dialogs_corpus.zip\n",
        "!mkdir datasets\n",
        "!mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets\n",
        "!mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWIaby8gW_B5"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.utils.data\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC80fs_rXEm8"
      },
      "source": [
        "## 1) Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSaDBwd6XN5M"
      },
      "outputs": [],
      "source": [
        "# data processing\n",
        "max_len = 25\n",
        "\n",
        "def remove_punc(string):\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    no_punct = \"\"\n",
        "    for char in string:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char  # space is also a character\n",
        "    return no_punct.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0HEfBglXX82"
      },
      "outputs": [],
      "source": [
        "corpus_movie_conv = './datasets/movie_conversations.txt'\n",
        "corpus_movie_lines = './datasets/movie_lines.txt'\n",
        "with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
        "    conv = c.readlines()\n",
        "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
        "    lines = l.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check first 5 lines"
      ],
      "metadata": {
        "id": "WlL25NatYqqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line_iter = iter(lines)\n",
        "lines_dic = {}\n",
        "\n",
        "for _ in range(5):\n",
        "    try:\n",
        "        line = next(line_iter)\n",
        "        print(line)\n",
        "\n",
        "    except StopIteration:\n",
        "        break"
      ],
      "metadata": {
        "id": "cSshaXM8Yi7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_iter = iter(conv)\n",
        "conv_dic = {}\n",
        "\n",
        "for _ in range(5):\n",
        "    try:\n",
        "        line = next(conv_iter)\n",
        "        print(line)\n",
        "\n",
        "    except StopIteration:\n",
        "        break"
      ],
      "metadata": {
        "id": "jbwYn-nwZSus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract text\n",
        "lines_dic = {}\n",
        "for line in lines:\n",
        "    objects = line.split(\" +++$+++ \")\n",
        "    lines_dic[objects[0]] = objects[-1]\n",
        "\n",
        "# generate question answer pairs\n",
        "pairs = []\n",
        "for con in conv:\n",
        "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
        "    for i in range(len(ids)):\n",
        "        qa_pairs = []\n",
        "\n",
        "        if i == len(ids) - 1:\n",
        "            break\n",
        "        print(lines_dic[ids[1]])\n",
        "        first = remove_punc(lines_dic[ids[i]].strip())\n",
        "        second = remove_punc(lines_dic[ids[i+1]].strip())\n",
        "        qa_pairs.append(first.split()[:max_len])\n",
        "        qa_pairs.append(second.split()[:max_len])\n",
        "        pairs.append(qa_pairs)\n",
        "# sample\n",
        "print(pairs[20])"
      ],
      "metadata": {
        "id": "zz7TowM_Y9mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_word_freq = 5\n",
        "\n",
        "word_freq = Counter()\n",
        "for pair in pairs:\n",
        "    word_freq.update(pair[0])\n",
        "    word_freq.update(pair[1])"
      ],
      "metadata": {
        "id": "JdZb523deH1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "min_word_freq: sets a threshold to ignore rare words (those appearing â‰¤ 5\n",
        "times).\n",
        "\n",
        "word_freq: a Counter() that keeps track of how often each word appears in questions and answers.\n",
        "\n",
        "pair[0]: the question (list of words).\n",
        "\n",
        "pair[1]: the reply (list of words).\n",
        "\n",
        "update(...): adds to the count for each word.\n",
        "\n"
      ],
      "metadata": {
        "id": "mwDaZW0sd2EU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create the Vocabulary (Word Map)\n",
        "Filters out rare words.\n",
        "\n",
        "word_map: assigns a unique integer ID to each word, starting from 1.\n",
        "\n",
        "Then, adds special tokens:"
      ],
      "metadata": {
        "id": "3JGylhcjeTvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
        "word_map = {k: v + 1 for v, k in enumerate(words)}"
      ],
      "metadata": {
        "id": "cU8MGpWmdzbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ Special tokens are useful for sequence models:\n",
        "\n",
        "$<unk>$ for unknown words\n",
        "\n",
        "$<start>$ & $<end>$ for indicating sequence boundaries\n",
        "\n",
        "$<pad>$ ensures sequences are same length (batching)"
      ],
      "metadata": {
        "id": "PlG97pnxexpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_map['<unk>'] = None\n",
        "word_map['<start>'] = None\n",
        "word_map['<end>'] = None\n",
        "word_map['<pad>'] = None\n",
        "\n",
        "print(\"Total words are: {}\".format(len(word_map)))\n",
        "\n"
      ],
      "metadata": {
        "id": "oO9OVOH7cfXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ 3. Encoding Functions\n",
        "Converts list of words â†’ list of integers (based on word_map).\n",
        "\n",
        "Question Encoder:\n",
        "Replaces each word with its index from word_map.\n",
        "\n",
        "Pads to fixed length max_len."
      ],
      "metadata": {
        "id": "HcRswlE7fSXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode sentences based on word map\n",
        "def encode_question(words, word_map):\n",
        "    enc_c = None\n",
        "    return enc_c"
      ],
      "metadata": {
        "id": "UctsaOQJfFea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds $<start>$ and $<end>$ tokens.\n",
        "\n",
        "Pads to max_len.\n",
        "\n",
        "ðŸ’¡ max_len it's the fixed length of sequences for the model."
      ],
      "metadata": {
        "id": "xLLJPJuYfkq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_reply(words, word_map):\n",
        "    enc_c = None\n",
        "    return enc_c"
      ],
      "metadata": {
        "id": "Vx2U2Yr_fb3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applies the encoders to all question-answer pairs.\n",
        "\n",
        "Final result: pairs_encoded is a list of [encoded_question, encoded_reply] pairs â€” ready for training."
      ],
      "metadata": {
        "id": "-JJGFFouf7CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_encoded = []\n",
        "for pair in pairs:\n",
        "    qus = encode_question(pair[0], word_map)\n",
        "    ans = encode_reply(pair[1], word_map)\n",
        "    pairs_encoded.append([qus, ans])"
      ],
      "metadata": {
        "id": "7YiKzvrxf59V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset and dataloader\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, pairs):\n",
        "\n",
        "        self.pairs = pairs\n",
        "        self.dataset_size = len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        question = torch.LongTensor(self.pairs[i][0])\n",
        "        reply = torch.LongTensor(self.pairs[i][1])\n",
        "        return question, reply\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "train_loader = DataLoader(Dataset(pairs_encoded), batch_size=32, shuffle=True, pin_memory=True)\n",
        "question, reply = next(iter(train_loader))\n",
        "print(\"Question: \", question.size())\n",
        "print(\"Answer: \", reply.size())"
      ],
      "metadata": {
        "id": "AOpQIwlpgNPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Masking\n",
        "\n",
        "Mask all the pad tokens (value 0) in the batch to ensure the model does not treat padding as input.\n",
        "\n",
        "Look-ahead Mask to mask the future tokens in a sequence. We also mask out pad tokens. i.e. To predict the third word, only the first and second word will be used"
      ],
      "metadata": {
        "id": "rtwlRd5PgbgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For training a Transformer, we need to mask:\n",
        "\n",
        "Padding tokens (so the model ignores them)\n",
        "\n",
        "Future tokens in the decoder input (so it can't \"cheat\" and see the next word)\n",
        "\n",
        "Target tokens for calculating loss"
      ],
      "metadata": {
        "id": "K5M2iXlfrimA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_test = torch.triu(torch.ones(5, 5)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "print(mask_test.unsqueeze(0))"
      ],
      "metadata": {
        "id": "tQ_R9zgFrhxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the decoder can only attend to previous and current tokens, not future ones."
      ],
      "metadata": {
        "id": "KrfMvjSBsDgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. âœ… question_mask\n",
        "\n",
        "question != 0: masks out <pad> tokens (index 0)\n",
        "\n",
        "Unsqueezes shape to (batch_size, 1, 1, seq_len) for broadcasting in multi-head attention"
      ],
      "metadata": {
        "id": "w6qwjjv6sdDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. âœ… reply_input_mask\n",
        "\n",
        "Removes padding and applies the triangular future mask\n",
        "\n",
        "Final shape: (batch_size, 1, seq_len, seq_len)\n",
        "\n",
        "This ensures the decoder can only attend to valid previous words."
      ],
      "metadata": {
        "id": "WQVDm-jCsd9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. âœ… reply_target_mask\n",
        "\n",
        "Used during training for loss calculation\n",
        "\n",
        "Shape: (batch_size, seq_len)\n",
        "\n",
        "Marks valid tokens (ignores <pad>)\n",
        "\n"
      ],
      "metadata": {
        "id": "xf5Z6-yds0X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create mask\n",
        "def create_masks(question, reply_input, reply_target, device='cpu'):\n",
        "\n",
        "    def subsequent_mask(size):\n",
        "        # (max_words, max_words)\n",
        "        # binary triangle\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        # (1, max_words, max_words)\n",
        "        return mask.unsqueeze(0)\n",
        "\n",
        "    # boolean(m, max_words)\n",
        "    question_mask = None\n",
        "\n",
        "    # (m, 1, 1, max_words)\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = None\n",
        "    # boolean(m, max_words)\n",
        "    reply_input_mask = None\n",
        "    # (m, 1, max_words)\n",
        "    reply_input_mask = None\n",
        "\n",
        "    # only include triangle and non-pad token\n",
        "    # (m, max_words, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data)\n",
        "    # (batch_size, max_words)\n",
        "    reply_target_mask = reply_target != 0\n",
        "\n",
        "    return question_mask, reply_input_mask, reply_target_mask"
      ],
      "metadata": {
        "id": "B9c2Z34xgZnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is typical in sequence modeling where input is like:\n",
        "\n",
        "\n",
        "**Input to decoder**:      $<start>$ how are\n",
        "<br>\n",
        "**Target to predict**:     how are you $<end>$"
      ],
      "metadata": {
        "id": "SZCdOa_1iheI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reply_input = reply[:, :-1]\n",
        "reply_target = reply[:, 1:]\n",
        "print('Reply Target Size: ', reply_target.size())\n",
        "\n",
        "# Create mask and add dimensions\n",
        "question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
        "print('question_mask Size: ', question_mask.size())\n",
        "print('reply_input_mask Size: ', reply_input_mask.size())\n",
        "print('reply_target_mask Size: ', reply_target_mask.size())"
      ],
      "metadata": {
        "id": "qNfp7PBEt_QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Reply Target Size:  torch.Size([32, 26])\n",
        "question_mask Size:  torch.Size([32, 1, 1, 25])\n",
        "reply_input_mask Size:  torch.Size([32, 1, 26, 26])\n",
        "reply_target_mask Size:  torch.Size([32, 26])\n",
        "```"
      ],
      "metadata": {
        "id": "9CxtD-QWuCLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Positional Embedding\n",
        "3.1 Positional Embedding\n",
        "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence.\n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other.\n",
        "\n",
        "Implement based on [this nb](https://https://github.com/tannisthamaiti/AIWeekend-Project/blob/main/Positional_Encoding.ipynb)"
      ],
      "metadata": {
        "id": "jVd8QFvRuwFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ðŸ” forward(self, embedding, layer_idx)\n",
        "embedding: the input tensor, either word indices (if layer_idx == 0) or already embedded vectors.\n",
        "\n",
        "layer_idx: the current layer number in the Transformer (used for layer-wise encoding).\n",
        "\n",
        "âœ… 1. Embedding the input if it's the first layer\n",
        "\n",
        "Applies word embedding only once, at the first layer.\n",
        "\n",
        "Scales the embedding vector (standard in Transformer) to stabilize training.\n",
        "\n",
        "âœ… 2. Add positional encoding\n",
        "\n",
        "self.pe: shape (1, max_len, d_model)\n",
        "\n",
        "Adds position information (word position in the sequence).\n",
        "\n",
        "Automatically broadcasts across the batch dimension.\n",
        "\n",
        "âœ… 3. Add layer encoding (temporal embedding per layer)\n",
        "\n",
        "self.te: shape (1, num_layers, d_model)\n",
        "\n",
        "This allows the model to differentiate between layers (like a form of layer-specific bias).\n",
        "\n",
        "Repeats the layer encoding across the sequence length dimension to match shape.\n",
        "\n",
        "âœ… 4. Dropout\n",
        "\n",
        "Adds regularization to prevent overfitting."
      ],
      "metadata": {
        "id": "QfinI2EgxEkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Embedding\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len=50, num_layers=6):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = None\n",
        "        self.embed = None\n",
        "        # (1, max_len, d_model)\n",
        "        self.pe = self.create_positional_encoding(None, None)\n",
        "        # (1, num_layers, d_model)\n",
        "        self.te = self.create_positional_encoding(None, None)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def create_positional_encoding(self, max_len, d_model, device='cpu'):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        # for each position of the word\n",
        "        for pos in range(max_len):\n",
        "            # for each dimension of the each position\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        # (1, max_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, embedding, layer_idx):\n",
        "        # create embed weight during first layer\n",
        "        # (m, max_len) --> (m, max_len, d_model)\n",
        "        if layer_idx == 0:\n",
        "            # scaling helps in stabilizing and improving the convergence properties\n",
        "            embedding = None\n",
        "\n",
        "        ### Positional Embedding\n",
        "        # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        # (m, max_len, d_model)\n",
        "        embedding += None\n",
        "\n",
        "        # te: (1, num_layers, d_model) --> (1, 1, d_model) --> (1, max_len, d_model)\n",
        "        # (m, max_len, d_model)\n",
        "        embedding += None\n",
        "        embedding = None\n",
        "        return embedding"
      ],
      "metadata": {
        "id": "iYt_u9Hzuuxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = Embeddings(len(word_map), 512)\n",
        "result = embed_model.forward(question, 0)\n",
        "print(result.size())"
      ],
      "metadata": {
        "id": "pxdtnd5Aigd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "torch.Size([32, 25, 512])\n",
        "```"
      ],
      "metadata": {
        "id": "XUH1MxRawhWI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MbMMr1xIwgfw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8oLeNouo0Xp/hnP1ELIsW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}