{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tannisthamaiti/AIWeekend-Project/blob/main/Transformer/Transformer_chatbot_Question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wurLjheWz0x"
      },
      "outputs": [],
      "source": [
        "!pip install datasets torch transformers\n",
        "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
        "!unzip -qq cornell_movie_dialogs_corpus.zip\n",
        "!rm cornell_movie_dialogs_corpus.zip\n",
        "!mkdir datasets\n",
        "!mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets\n",
        "!mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWIaby8gW_B5"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.utils.data\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC80fs_rXEm8"
      },
      "source": [
        "## 1) Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSaDBwd6XN5M"
      },
      "outputs": [],
      "source": [
        "# data processing\n",
        "max_len = 25\n",
        "\n",
        "def remove_punc(string):\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    no_punct = \"\"\n",
        "    for char in string:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char  # space is also a character\n",
        "    return no_punct.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0HEfBglXX82"
      },
      "outputs": [],
      "source": [
        "corpus_movie_conv = './datasets/movie_conversations.txt'\n",
        "corpus_movie_lines = './datasets/movie_lines.txt'\n",
        "with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
        "    conv = c.readlines()\n",
        "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
        "    lines = l.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check first 5 lines"
      ],
      "metadata": {
        "id": "WlL25NatYqqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line_iter = iter(lines)\n",
        "lines_dic = {}\n",
        "\n",
        "for _ in range(5):\n",
        "    try:\n",
        "        line = next(line_iter)\n",
        "        print(line)\n",
        "\n",
        "    except StopIteration:\n",
        "        break"
      ],
      "metadata": {
        "id": "cSshaXM8Yi7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_iter = iter(conv)\n",
        "conv_dic = {}\n",
        "\n",
        "for _ in range(5):\n",
        "    try:\n",
        "        line = next(conv_iter)\n",
        "        print(line)\n",
        "\n",
        "    except StopIteration:\n",
        "        break"
      ],
      "metadata": {
        "id": "jbwYn-nwZSus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract text\n",
        "lines_dic = {}\n",
        "for line in lines:\n",
        "    objects = line.split(\" +++$+++ \")\n",
        "    lines_dic[objects[0]] = objects[-1]\n",
        "\n",
        "# generate question answer pairs\n",
        "pairs = []\n",
        "for con in conv:\n",
        "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
        "    for i in range(len(ids)):\n",
        "        qa_pairs = []\n",
        "\n",
        "        if i == len(ids) - 1:\n",
        "            break\n",
        "        print(lines_dic[ids[1]])\n",
        "        first = remove_punc(lines_dic[ids[i]].strip())\n",
        "        second = remove_punc(lines_dic[ids[i+1]].strip())\n",
        "        qa_pairs.append(first.split()[:max_len])\n",
        "        qa_pairs.append(second.split()[:max_len])\n",
        "        pairs.append(qa_pairs)\n",
        "# sample\n",
        "print(pairs[20])"
      ],
      "metadata": {
        "id": "zz7TowM_Y9mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_word_freq = 5\n",
        "\n",
        "word_freq = Counter()\n",
        "for pair in pairs:\n",
        "    word_freq.update(pair[0])\n",
        "    word_freq.update(pair[1])"
      ],
      "metadata": {
        "id": "JdZb523deH1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "min_word_freq: sets a threshold to ignore rare words (those appearing â‰¤ 5\n",
        "times).\n",
        "\n",
        "word_freq: a Counter() that keeps track of how often each word appears in questions and answers.\n",
        "\n",
        "pair[0]: the question (list of words).\n",
        "\n",
        "pair[1]: the reply (list of words).\n",
        "\n",
        "update(...): adds to the count for each word.\n",
        "\n"
      ],
      "metadata": {
        "id": "mwDaZW0sd2EU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create the Vocabulary (Word Map)\n",
        "Filters out rare words.\n",
        "\n",
        "word_map: assigns a unique integer ID to each word, starting from 1.\n",
        "\n",
        "Then, adds special tokens:"
      ],
      "metadata": {
        "id": "3JGylhcjeTvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
        "word_map = {k: v + 1 for v, k in enumerate(words)}"
      ],
      "metadata": {
        "id": "cU8MGpWmdzbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ Special tokens are useful for sequence models:\n",
        "\n",
        "$<unk>$ for unknown words\n",
        "\n",
        "$<start>$ & $<end>$ for indicating sequence boundaries\n",
        "\n",
        "$<pad>$ ensures sequences are same length (batching)"
      ],
      "metadata": {
        "id": "PlG97pnxexpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_map['<unk>'] = None\n",
        "word_map['<start>'] = None\n",
        "word_map['<end>'] = None\n",
        "word_map['<pad>'] = None\n",
        "\n",
        "print(\"Total words are: {}\".format(len(word_map)))\n",
        "\n"
      ],
      "metadata": {
        "id": "oO9OVOH7cfXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ 3. Encoding Functions\n",
        "Converts list of words â†’ list of integers (based on word_map).\n",
        "\n",
        "Question Encoder:\n",
        "Replaces each word with its index from word_map.\n",
        "\n",
        "Pads to fixed length max_len."
      ],
      "metadata": {
        "id": "HcRswlE7fSXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode sentences based on word map\n",
        "def encode_question(words, word_map):\n",
        "    enc_c = None\n",
        "    return enc_c"
      ],
      "metadata": {
        "id": "UctsaOQJfFea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds $<start>$ and $<end>$ tokens.\n",
        "\n",
        "Pads to max_len.\n",
        "\n",
        "ðŸ’¡ max_len it's the fixed length of sequences for the model."
      ],
      "metadata": {
        "id": "xLLJPJuYfkq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_reply(words, word_map):\n",
        "    enc_c = None\n",
        "    return enc_c"
      ],
      "metadata": {
        "id": "Vx2U2Yr_fb3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applies the encoders to all question-answer pairs.\n",
        "\n",
        "Final result: pairs_encoded is a list of [encoded_question, encoded_reply] pairs â€” ready for training."
      ],
      "metadata": {
        "id": "-JJGFFouf7CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_encoded = []\n",
        "for pair in pairs:\n",
        "    qus = encode_question(pair[0], word_map)\n",
        "    ans = encode_reply(pair[1], word_map)\n",
        "    pairs_encoded.append([qus, ans])"
      ],
      "metadata": {
        "id": "7YiKzvrxf59V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset and dataloader\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, pairs):\n",
        "\n",
        "        self.pairs = pairs\n",
        "        self.dataset_size = len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        question = torch.LongTensor(self.pairs[i][0])\n",
        "        reply = torch.LongTensor(self.pairs[i][1])\n",
        "        return question, reply\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "train_loader = DataLoader(Dataset(pairs_encoded), batch_size=32, shuffle=True, pin_memory=True)\n",
        "question, reply = next(iter(train_loader))\n",
        "print(\"Question: \", question.size())\n",
        "print(\"Answer: \", reply.size())"
      ],
      "metadata": {
        "id": "AOpQIwlpgNPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Masking\n",
        "\n",
        "Mask all the pad tokens (value 0) in the batch to ensure the model does not treat padding as input.\n",
        "\n",
        "Look-ahead Mask to mask the future tokens in a sequence. We also mask out pad tokens. i.e. To predict the third word, only the first and second word will be used"
      ],
      "metadata": {
        "id": "rtwlRd5PgbgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For training a Transformer, we need to mask:\n",
        "\n",
        "Padding tokens (so the model ignores them)\n",
        "\n",
        "Future tokens in the decoder input (so it can't \"cheat\" and see the next word)\n",
        "\n",
        "Target tokens for calculating loss"
      ],
      "metadata": {
        "id": "K5M2iXlfrimA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_test = torch.triu(torch.ones(5, 5)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "print(mask_test.unsqueeze(0))"
      ],
      "metadata": {
        "id": "tQ_R9zgFrhxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the decoder can only attend to previous and current tokens, not future ones."
      ],
      "metadata": {
        "id": "KrfMvjSBsDgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. âœ… question_mask\n",
        "\n",
        "question != 0: masks out <pad> tokens (index 0)\n",
        "\n",
        "Unsqueezes shape to (batch_size, 1, 1, seq_len) for broadcasting in multi-head attention"
      ],
      "metadata": {
        "id": "w6qwjjv6sdDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. âœ… reply_input_mask\n",
        "\n",
        "Removes padding and applies the triangular future mask\n",
        "\n",
        "Final shape: (batch_size, 1, seq_len, seq_len)\n",
        "\n",
        "This ensures the decoder can only attend to valid previous words."
      ],
      "metadata": {
        "id": "WQVDm-jCsd9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. âœ… reply_target_mask\n",
        "\n",
        "Used during training for loss calculation\n",
        "\n",
        "Shape: (batch_size, seq_len)\n",
        "\n",
        "Marks valid tokens (ignores <pad>)\n",
        "\n"
      ],
      "metadata": {
        "id": "xf5Z6-yds0X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create mask\n",
        "def create_masks(question, reply_input, reply_target, device='cpu'):\n",
        "\n",
        "    def subsequent_mask(size):\n",
        "        # (max_words, max_words)\n",
        "        # binary triangle\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        # (1, max_words, max_words)\n",
        "        return mask.unsqueeze(0)\n",
        "\n",
        "    # boolean(m, max_words)\n",
        "    question_mask = None\n",
        "\n",
        "    # (m, 1, 1, max_words)\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = None\n",
        "    # boolean(m, max_words)\n",
        "    reply_input_mask = None\n",
        "    # (m, 1, max_words)\n",
        "    reply_input_mask = None\n",
        "\n",
        "    # only include triangle and non-pad token\n",
        "    # (m, max_words, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data)\n",
        "    # (batch_size, max_words)\n",
        "    reply_target_mask = reply_target != 0\n",
        "\n",
        "    return question_mask, reply_input_mask, reply_target_mask"
      ],
      "metadata": {
        "id": "B9c2Z34xgZnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is typical in sequence modeling where input is like:\n",
        "\n",
        "\n",
        "**Input to decoder**:      $<start>$ how are\n",
        "<br>\n",
        "**Target to predict**:     how are you $<end>$"
      ],
      "metadata": {
        "id": "SZCdOa_1iheI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reply_input = reply[:, :-1]\n",
        "reply_target = reply[:, 1:]\n",
        "print('Reply Target Size: ', reply_target.size())\n",
        "\n",
        "# Create mask and add dimensions\n",
        "question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
        "print('question_mask Size: ', question_mask.size())\n",
        "print('reply_input_mask Size: ', reply_input_mask.size())\n",
        "print('reply_target_mask Size: ', reply_target_mask.size())"
      ],
      "metadata": {
        "id": "qNfp7PBEt_QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Reply Target Size:  torch.Size([32, 26])\n",
        "question_mask Size:  torch.Size([32, 1, 1, 25])\n",
        "reply_input_mask Size:  torch.Size([32, 1, 26, 26])\n",
        "reply_target_mask Size:  torch.Size([32, 26])\n",
        "```"
      ],
      "metadata": {
        "id": "9CxtD-QWuCLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Positional Embedding\n",
        "3.1 Positional Embedding\n",
        "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence.\n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other.\n",
        "\n",
        "Implement based on [this nb](https://https://github.com/tannisthamaiti/AIWeekend-Project/blob/main/Positional_Encoding.ipynb)"
      ],
      "metadata": {
        "id": "jVd8QFvRuwFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ðŸ” forward(self, embedding, layer_idx)\n",
        "embedding: the input tensor, either word indices (if layer_idx == 0) or already embedded vectors.\n",
        "\n",
        "layer_idx: the current layer number in the Transformer (used for layer-wise encoding).\n",
        "\n",
        "âœ… 1. Embedding the input if it's the first layer\n",
        "\n",
        "Applies word embedding only once, at the first layer.\n",
        "\n",
        "Scales the embedding vector (standard in Transformer) to stabilize training.\n",
        "\n",
        "âœ… 2. Add positional encoding\n",
        "\n",
        "self.pe: shape (1, max_len, d_model)\n",
        "\n",
        "Adds position information (word position in the sequence).\n",
        "\n",
        "Automatically broadcasts across the batch dimension.\n",
        "\n",
        "âœ… 3. Add layer encoding (temporal embedding per layer)\n",
        "\n",
        "self.te: shape (1, num_layers, d_model)\n",
        "\n",
        "This allows the model to differentiate between layers (like a form of layer-specific bias).\n",
        "\n",
        "Repeats the layer encoding across the sequence length dimension to match shape.\n",
        "\n",
        "âœ… 4. Dropout\n",
        "\n",
        "Adds regularization to prevent overfitting."
      ],
      "metadata": {
        "id": "QfinI2EgxEkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Embedding\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len=50, num_layers=6):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = None\n",
        "        self.embed = None\n",
        "        # (1, max_len, d_model)\n",
        "        self.pe = self.create_positional_encoding(None, None)\n",
        "        # (1, num_layers, d_model)\n",
        "        self.te = self.create_positional_encoding(None, None)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def create_positional_encoding(self, max_len, d_model, device='cpu'):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        # for each position of the word\n",
        "        for pos in range(max_len):\n",
        "            # for each dimension of the each position\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        # (1, max_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, embedding, layer_idx):\n",
        "        # create embed weight during first layer\n",
        "        # (m, max_len) --> (m, max_len, d_model)\n",
        "        if layer_idx == 0:\n",
        "            # scaling helps in stabilizing and improving the convergence properties\n",
        "            embedding = None\n",
        "\n",
        "        ### Positional Embedding\n",
        "        # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        # (m, max_len, d_model)\n",
        "        embedding += None\n",
        "\n",
        "        # te: (1, num_layers, d_model) --> (1, 1, d_model) --> (1, max_len, d_model)\n",
        "        # (m, max_len, d_model)\n",
        "        embedding += None\n",
        "        embedding = None\n",
        "        return embedding"
      ],
      "metadata": {
        "id": "iYt_u9Hzuuxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = Embeddings(len(word_map), 512)\n",
        "result = embed_model.forward(question, 0)\n",
        "print(result.size())"
      ],
      "metadata": {
        "id": "pxdtnd5Aigd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "torch.Size([32, 25, 512])\n",
        "```"
      ],
      "metadata": {
        "id": "XUH1MxRawhWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Attention\n",
        "![Alt Text](https://github.com/tannisthamaiti/AIWeekend-Project/blob/main/images/4a.jpg?raw=true)\n",
        "![Alt Text](https://github.com/tannisthamaiti/AIWeekend-Project/blob/main/images/3a.jpg?raw=true)\n",
        "\n",
        "## Multi-Head Attention\n",
        "![Alt Text](https://github.com/tannisthamaiti/AIWeekend-Project/blob/main/images/7.jpg?raw=true)\n",
        "Multi-head attention consists of four parts:\n",
        "\n",
        "Linear layers and split into heads.\n",
        "\n",
        "Scaled dot-product attention.\n",
        "\n",
        "Concatenation of heads.\n",
        "\n",
        "Final linear layer.\n",
        "\n",
        "![Alt Text](https://github.com/tannisthamaiti/AIWeekend-Project/blob/main/images/6.jpg?raw=true)\n"
      ],
      "metadata": {
        "id": "zBOVjqxw22g3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "        self.row_dim = row_dim\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "\n",
        "    ## The only change from SelfAttention and attention is that\n",
        "    ## now we expect 3 sets of encodings to be passed in...\n",
        "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
        "        ## ...and we pass those sets of encodings to the various weight matrices.\n",
        "        q = self.W_q(encodings_for_q)\n",
        "        k = self.W_k(encodings_for_k)\n",
        "        v = self.W_v(encodings_for_v)\n",
        "\n",
        "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
        "\n",
        "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
        "\n",
        "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
        "\n",
        "        attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "        return attention_scores"
      ],
      "metadata": {
        "id": "f8rXqeYz5Hgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1,\n",
        "                 num_heads=1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        ## create a bunch of attention heads\n",
        "        self.heads = nn.ModuleList(\n",
        "            [Attention(d_model, row_dim, col_dim)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "    def forward(self,\n",
        "                encodings_for_q,\n",
        "                encodings_for_k,\n",
        "                encodings_for_v):\n",
        "\n",
        "        ## run the data through all of the attention heads\n",
        "        return torch.cat([head(encodings_for_q,\n",
        "                               encodings_for_k,\n",
        "                               encodings_for_v)\n",
        "                          for head in self.heads], dim=self.col_dim)"
      ],
      "metadata": {
        "id": "MbMMr1xIwgfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 FeedForward Layer\n",
        "Network with one hidden layer that applies a non-linear activation function (GELU) to the output of the first linear layer"
      ],
      "metadata": {
        "id": "VlKXP9Gg5vR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, middle_dim=2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.dropout = None\n",
        "        self.activation = torch.nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = None\n",
        "        out = None\n",
        "        return out"
      ],
      "metadata": {
        "id": "QUhAXneR5tva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Encoder Layer\n",
        "Each encoder layer consists of sublayers:\n",
        "\n",
        "Multi-head attention (with padding mask)\n",
        "\n",
        "2 dense layers followed by dropout\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
        "\n",
        "The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis.\n",
        "\n",
        "![Alt txt](https://github.com/tannisthamaiti/AIWeekend-Project/blob/main/images/8.png?raw=true)\n"
      ],
      "metadata": {
        "id": "zpzOwTza7UaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim=2048, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention\n",
        "        self.self_attn = None\n",
        "        # Feed-forward network\n",
        "        self.feed_forward = None\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = None\n",
        "        self.norm2 = None\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = None\n",
        "        self.dropout2 = None\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # --- Self-Attention sublayer with residual and layer norm ---\n",
        "        attn_output = self.self_attn(x, x, x)  # Self-attention\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # --- Feed-forward sublayer with residual and layer norm ---\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = None\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mFZx3Gcb69so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“¥ Expected Input/Output:\n",
        "Input: x of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "Mask: shape (batch_size, 1, 1, seq_len) (optional, e.g., for padding)\n",
        "\n",
        "Output: same shape as input"
      ],
      "metadata": {
        "id": "rB6XhvsQ8ABf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy configuration\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "d_model = 4\n",
        "num_heads = 2\n",
        "ff_dim = 16\n",
        "\n",
        "# Dummy input tensor (batch_size, seq_len, d_model)\n",
        "x = torch.rand(batch_size, seq_len, d_model)\n",
        "\n",
        "# Dummy padding mask: shape (batch_size, 1, 1, seq_len)\n",
        "# This example assumes all tokens are valid (no padding)\n",
        "padding_mask = torch.ones(batch_size, 1, 1, seq_len).bool()\n",
        "\n",
        "# Instantiate the EncoderLayer\n",
        "encoder_layer = EncoderLayer(d_model=d_model, num_heads=num_heads, ff_dim=ff_dim)\n",
        "\n",
        "# Forward pass\n",
        "output = encoder_layer(x, mask=padding_mask)\n",
        "\n",
        "# Check results\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Output tensor:\\n\", output)"
      ],
      "metadata": {
        "id": "_mpAKVn38koC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Input shape: torch.Size([2, 5, 4])\n",
        "Output shape: torch.Size([2, 5, 4])\n",
        "Output tensor:\n",
        "tensor([...])\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "FXM7FZlL8luJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.5 Decoder Layer\n",
        "Each decoder layer consists of sublayers:\n",
        "\n",
        "1.Masked multi-head attention (with look ahead mask and padding mask)\n",
        "<br>\n",
        "2.Multi-head attention (with padding mask). value and key receive the encoder output as inputs. query receives the output from the masked multi-head attention sublayer.\n",
        "<br>\n",
        "3.2 dense layers followed by dropout\n",
        "<br>\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
        "\n",
        "As query receives the output from decoder's first attention block, and key receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section.\n",
        "\n",
        "âœ… DecoderLayer Implementation\n",
        "\n",
        "1.Masked Multi-Head Self-Attention (with future token masking)\n",
        "\n",
        "2.Multi-Head Encoder-Decoder Attention (cross attention)\n",
        "\n",
        "3.Feed-Forward Network\n",
        "\n",
        "4.Residual Connections + Layer Normalization"
      ],
      "metadata": {
        "id": "JpZZH7PvAnRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim=2048, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Masked self-attention (with future mask)\n",
        "        self.self_attn = None\n",
        "        # Encoder-decoder attention\n",
        "        self.cross_attn = None\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.feed_forward = None\n",
        "\n",
        "        # Layer norms\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropouts\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, self_attn_mask=None, cross_attn_mask=None):\n",
        "        # --- Masked Self-Attention ---\n",
        "        self_attn_output = self.self_attn(x, x, x)\n",
        "        x = None\n",
        "        # --- Encoder-Decoder Cross-Attention ---\n",
        "        cross_attn_output = self.cross_attn(x, enc_output, enc_output)\n",
        "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
        "\n",
        "        # --- Feed-Forward Network ---\n",
        "        ff_output = None\n",
        "        x = None\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zbsUqmKW7_AZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4grHwK2rVylgDSnrGDw6b",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}