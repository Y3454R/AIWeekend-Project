{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tannisthamaiti/AIWeekend-Project/blob/main/Transformer/Transformer_chatbot_complex_movie_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBbUJEFwbZ4V",
        "outputId": "963b659e-040a-4bcc-aa4f-fb42ca4d7c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "--2025-05-15 15:07:22--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.53\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip [following]\n",
            "--2025-05-15 15:07:23--  https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‚Äòcornell_movie_dialogs_corpus.zip‚Äô\n",
            "\n",
            "cornell_movie_dialo 100%[===================>]   9.46M  13.7MB/s    in 0.7s    \n",
            "\n",
            "2025-05-15 15:07:24 (13.7 MB/s) - ‚Äòcornell_movie_dialogs_corpus.zip‚Äô saved [9916637/9916637]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets torch transformers\n",
        "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
        "!unzip -qq cornell_movie_dialogs_corpus.zip\n",
        "!rm cornell_movie_dialogs_corpus.zip\n",
        "!mkdir datasets\n",
        "!mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets\n",
        "!mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing\n",
        "\n",
        "This tutorial trains a Transformer model to be a chatbot. This is an advanced example that assumes knowledge of text generation, attention and transformer.\n",
        "\n",
        "We will use the conversations in movies and TV shows provided by Cornell Movie-Dialogs Corpus, which contains more than 220 thousands conversational exchanges between more than 10k pairs of movie characters, as our dataset.\n",
        "\n",
        "movie_conversations.txt contains list of the conversation IDs and movie_lines.text contains the text of assoicated with each conversation ID. For further information regarding the dataset, please check the README file in the zip file."
      ],
      "metadata": {
        "id": "v5pnboqLbvJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "import math\n"
      ],
      "metadata": {
        "id": "rrrHwFJOdAzi"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch ## torch let's us create tensors and also provides helper functions\n",
        "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
        "import torch.nn.functional as F # This gives us the softmax()\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset ## We'll store our data in DataLoaders\n",
        "from torch.optim import Adam\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "izzM6fe_oUnr"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(string):\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    no_punct = \"\"\n",
        "    for char in string:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char  # space is also a character\n",
        "    return no_punct.lower()"
      ],
      "metadata": {
        "id": "wG-lPDUJcQWu"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data processing\n",
        "max_len = 25\n",
        "\n",
        "corpus_movie_conv = './datasets/movie_conversations.txt'\n",
        "corpus_movie_lines = './datasets/movie_lines.txt'\n",
        "with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
        "    conv = c.readlines()\n",
        "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
        "    lines = l.readlines()\n",
        "\n",
        "# extract text\n",
        "lines_dic = {}\n",
        "for line in lines:\n",
        "    objects = line.split(\" +++$+++ \")\n",
        "    lines_dic[objects[0]] = objects[-1]"
      ],
      "metadata": {
        "id": "aeOfqGIvbtiS"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate question answer pairs\n",
        "pairs = []\n",
        "for con in conv:\n",
        "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
        "    for i in range(len(ids)):\n",
        "        qa_pairs = []\n",
        "\n",
        "        if i == len(ids) - 1:\n",
        "            break\n",
        "\n",
        "        first = remove_punc(lines_dic[ids[i]].strip())\n",
        "        second = remove_punc(lines_dic[ids[i+1]].strip())\n",
        "        qa_pairs.append(first.split()[:max_len])\n",
        "        qa_pairs.append(second.split()[:max_len])\n",
        "        qa_pairs.append(second.split()[:max_len])\n",
        "        pairs.append(qa_pairs)\n",
        "\n",
        "# sample\n",
        "print(pairs[20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XagHo-TccGk",
        "outputId": "174659c2-750a-4dfe-b68b-905c12019a4a"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'really', 'really', 'really', 'wanna', 'go', 'but', 'i', 'cant', 'not', 'unless', 'my', 'sister', 'goes'], ['im', 'workin', 'on', 'it', 'but', 'she', 'doesnt', 'seem', 'to', 'be', 'goin', 'for', 'him'], ['im', 'workin', 'on', 'it', 'but', 'she', 'doesnt', 'seem', 'to', 'be', 'goin', 'for', 'him']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding"
      ],
      "metadata": {
        "id": "t_Mr4nfjc2vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word map\n",
        "min_word_freq = 5\n",
        "\n",
        "word_freq = Counter()\n",
        "for pair in pairs:\n",
        "    word_freq.update(pair[0])\n",
        "    word_freq.update(pair[1])\n",
        "\n",
        "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
        "word_map = {k: v + 1 for v, k in enumerate(words)}\n",
        "word_map['<unk>'] = len(word_map) + 1\n",
        "word_map['<start>'] = len(word_map) + 1\n",
        "word_map['<end>'] = len(word_map) + 1\n",
        "word_map[''] = len(word_map) + 1\n",
        "word_map['<pad>'] = 0\n",
        "\n",
        "print(\"Total words are: {}\".format(len(word_map)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DwFdaN3crWj",
        "outputId": "1fc05098-d5b6-4536-e90b-5d792747317c"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words are: 18244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "# encode sentences based on word map\n",
        "def encode_question(words, word_map):\n",
        "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n",
        "    return enc_c\n",
        "\n",
        "def encode_reply_input(words, word_map):\n",
        "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words]\n",
        "    enc_c += [word_map['<pad>']] * (max_len - len(enc_c))\n",
        "    return enc_c\n",
        "\n",
        "def encode_reply(words, word_map):\n",
        "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']]\n",
        "    enc_c += [word_map['<pad>']] * (max_len - len(enc_c))\n",
        "    return enc_c\n",
        "\n",
        "\n",
        "pairs_encoded = []\n",
        "for pair in pairs:\n",
        "    qus = encode_question(pair[0], word_map)\n",
        "    ans_input = encode_reply_input(pair[1], word_map)\n",
        "    ans = encode_reply(pair[2], word_map)\n",
        "    if len(qus) == 25 and len(ans_input) == 25 and len(ans) == 25:\n",
        "      pairs_encoded.append([qus, ans_input,ans])\n",
        "print(len(pairs_encoded))\n",
        "# Shuffle the dataset first (important!)\n",
        "shuffle(pairs_encoded)\n",
        "\n",
        "# Define split sizes\n",
        "total = len(pairs_encoded)\n",
        "train_size = int(0.7 * total)\n",
        "val_size = int(0.2998 * total)\n",
        "test_size = total - train_size - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_data = pairs_encoded[:train_size]\n",
        "val_data = pairs_encoded[train_size:train_size + val_size]\n",
        "test_data = pairs_encoded[train_size + val_size:]"
      ],
      "metadata": {
        "id": "O6o7K3vadT9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd50c21b-abcf-4b8c-f0d5-c2c8c7dea8ee"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "202213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset and dataloader\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, pairs):\n",
        "\n",
        "        self.pairs = pairs\n",
        "        self.dataset_size = len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        question = torch.LongTensor(self.pairs[i][0])\n",
        "        reply_input = torch.LongTensor(self.pairs[i][1])\n",
        "        reply = torch.LongTensor(self.pairs[i][2])\n",
        "        return question, reply_input, reply\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "\n",
        "#train_loader = DataLoader(Dataset(pairs_encoded), batch_size=32, shuffle=True, pin_memory=True)\n",
        "train_loader = DataLoader(Dataset(train_data), batch_size=32, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(Dataset(val_data), batch_size=32, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(Dataset(test_data), batch_size=1, shuffle=False, pin_memory=True)\n",
        "question, reply_input, reply = next(iter(train_loader))\n",
        "print(\"Question: \", question.size())\n",
        "print(\"Answer: \", reply_input.size())\n",
        "print(\"Answer: \", reply.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYN8aEqWdiaC",
        "outputId": "d472aea4-4e93-461f-c857-1f49bca60f95"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  torch.Size([32, 25])\n",
            "Answer:  torch.Size([32, 25])\n",
            "Answer:  torch.Size([32, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=2, max_len=25):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
        "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
        "        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) ## every other column, starting with the 1st, has sin() values\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) ## every other column, starting with the 2nd, has cos() values\n",
        "\n",
        "        ## Now we \"register 'pe'.\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, word_embeddings):\n",
        "\n",
        "        return word_embeddings + self.pe[:word_embeddings.size(0), :]\n"
      ],
      "metadata": {
        "id": "hHr9QLj2eDa6"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, d_model=2,row_dim=0,col_dim=1):\n",
        "      super().__init__()\n",
        "      self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "      self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "      self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "      self.row_dim = row_dim\n",
        "      self.col_dim = col_dim\n",
        "\n",
        "\n",
        "    ## The only change from SelfAttention and attention is that\n",
        "    ## now we expect 3 sets of encodings to be passed in...\n",
        "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
        "        ## ...and we pass those sets of encodings to the various weight matrices.\n",
        "        q = self.W_q(encodings_for_q)\n",
        "        k = self.W_k(encodings_for_k)\n",
        "        v = self.W_v(encodings_for_v)\n",
        "        # Transpose keys: [batch_size, d_model, seq_len_k]\n",
        "        if q.dim() == 3:  # [batch_size, seq_len, d_model]\n",
        "          k_t = k.transpose(1, 2)         # [batch_size, d_model, seq_len]\n",
        "          sims = torch.bmm(q, k_t)        # [batch_size, seq_len_q, seq_len_k]\n",
        "        else:  # assume [seq_len, d_model] (no batch)\n",
        "          k_t = k.transpose(0, 1)         # [d_model, seq_len]\n",
        "          sims = torch.matmul(q, k_t)     # [seq_len_q, seq_len_k]\n",
        "\n",
        "        scaled_sims = sims / torch.tensor(k.size(-1)**0.5)\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "            #print(\"mask.shape, scaled_sims.shape\",mask.shape, scaled_sims.shape )\n",
        "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
        "\n",
        "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
        "\n",
        "        attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "        return attention_scores"
      ],
      "metadata": {
        "id": "CNBTcs7nfMxJ"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_tokens=4, d_model=2, max_len=6):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.we = nn.Embedding(num_embeddings=num_tokens,\n",
        "                               embedding_dim=d_model)\n",
        "\n",
        "        self.pe = PositionEncoding(d_model=d_model,\n",
        "                                   max_len=max_len)\n",
        "\n",
        "        self.self_attention = Attention(d_model=d_model)\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "\n",
        "        device = self.we.weight.device  # ensure all tensors follow this device\n",
        "\n",
        "        token_ids = token_ids.to(device)\n",
        "        word_embeddings = self.we(token_ids)\n",
        "\n",
        "        position_encoded = self.pe(word_embeddings)\n",
        "\n",
        "        self_attention_values = self.self_attention(position_encoded,\n",
        "                                                    position_encoded,\n",
        "                                                    position_encoded,\n",
        "                                                    mask=None)\n",
        "\n",
        "\n",
        "        residual_connection_values = self.layernorm(position_encoded + self_attention_values)\n",
        "\n",
        "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
        "\n",
        "        return residual_connection_values, residual_connection_values\n"
      ],
      "metadata": {
        "id": "QiXmZbOgf7xn"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_tokens=4, d_model=2, max_len=6):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "        ## NOTE: In this simple example, we are just using a \"single layer\" decoder.\n",
        "        ##       If we wanted to have multiple layers of decoder, then we would\n",
        "        ##       take the output of one decoder module and use it as input to\n",
        "        ##       the next module.\n",
        "\n",
        "        self.we = nn.Embedding(num_embeddings=num_tokens,\n",
        "                               embedding_dim=d_model)\n",
        "\n",
        "        self.pe = PositionEncoding(d_model=d_model,\n",
        "                                   max_len=max_len)\n",
        "\n",
        "        self.self_attention = Attention(d_model=d_model)\n",
        "        self.cross_attention = Attention(d_model=d_model)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, token_ids,encoder_k, encoder_v):\n",
        "\n",
        "        token_ids = token_ids.to(self.we.weight.device)\n",
        "        word_embeddings = self.we(token_ids)\n",
        "        position_encoded = self.pe(word_embeddings)\n",
        "        if token_ids.dim() == 2:\n",
        "          mask = torch.tril(torch.ones((token_ids.size(1), token_ids.size(1))))\n",
        "          mask = mask.unsqueeze(0).expand(token_ids.size(0), -1, -1)  # [batch_size, seq_len, seq_len]\n",
        "        elif token_ids.dim() == 1:\n",
        "          mask = torch.tril(torch.ones((token_ids.size(0), token_ids.size(0))))\n",
        "        #mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=1))))\n",
        "        mask = mask == 0\n",
        "        mask = mask.to(self.we.weight.device)\n",
        "\n",
        "        mask_self_attention_values = self.self_attention(position_encoded,\n",
        "                                                    position_encoded,\n",
        "                                                    position_encoded,\n",
        "                                                    mask=mask)\n",
        "\n",
        "        residual_connection_values = self.layernorm1(position_encoded + mask_self_attention_values)\n",
        "        x_cross_att = self.cross_attention(residual_connection_values, encoder_k, encoder_v, mask=None)\n",
        "        x = self.layernorm2(residual_connection_values + x_cross_att)\n",
        "        fc_layer_output = self.fc_layer(x)\n",
        "\n",
        "\n",
        "\n",
        "        return fc_layer_output\n"
      ],
      "metadata": {
        "id": "xIRWAdRHgDAf"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## First, create a model from DecoderOnlyTransformer()\n",
        "model = Encoder(num_tokens=len(word_map), d_model=2, max_len=25)\n",
        "\n",
        "\n",
        "## Now create the input for the transformer...\n",
        "question_test = pairs[20][0]\n",
        "print(question_test)\n",
        "mapped_values = [word_map[word] for word in question_test]\n",
        "encoder_input = torch.tensor(mapped_values)\n",
        "print(\"encoder_input\", encoder_input.shape)\n",
        "\n",
        "## Now get get predictions from the model\n",
        "encoder_k, encoder_v = model(encoder_input)\n",
        "answer_test = pairs[20][1]\n",
        "print(answer_test)\n",
        "mapped_values = [word_map['<start>']]+[word_map[word] for word in answer_test]\n",
        "decoder_input = torch.tensor(mapped_values)\n",
        "print(\"decoder_input\", decoder_input.shape)\n",
        "decoder = Decoder(num_tokens=len(word_map), d_model=2, max_len=25)\n",
        "output = decoder(decoder_input, encoder_k, encoder_v)\n",
        "\n",
        "print(output)\n",
        "\n"
      ],
      "metadata": {
        "id": "BppoaYtDgjl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209ef699-8582-4c40-8180-a92593f98072"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'really', 'really', 'really', 'wanna', 'go', 'but', 'i', 'cant', 'not', 'unless', 'my', 'sister', 'goes']\n",
            "encoder_input torch.Size([14])\n",
            "['im', 'workin', 'on', 'it', 'but', 'she', 'doesnt', 'seem', 'to', 'be', 'goin', 'for', 'him']\n",
            "decoder_input torch.Size([14])\n",
            "tensor([[-0.2472,  0.3668,  0.7343,  ..., -0.2257, -0.6887, -0.7490],\n",
            "        [-0.2472,  0.3668,  0.7343,  ..., -0.2257, -0.6887, -0.7490],\n",
            "        [-0.2472,  0.3668,  0.7343,  ..., -0.2257, -0.6887, -0.7490],\n",
            "        ...,\n",
            "        [-0.1365,  0.8062, -0.9483,  ...,  0.1375,  1.8875,  0.3018],\n",
            "        [-0.2472,  0.3668,  0.7343,  ..., -0.2257, -0.6887, -0.7490],\n",
            "        [-0.1365,  0.8062, -0.9483,  ...,  0.1375,  1.8875,  0.3018]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparameters\n",
        "num_tokens = len(word_map)\n",
        "d_model = 2\n",
        "max_len = 25\n",
        "batch_size = 32\n",
        "num_epochs = 200\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "6KYuFoGRmJMO"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the transformer model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_tokens, d_model, max_len):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_tokens=num_tokens, d_model=d_model, max_len=max_len)\n",
        "        self.decoder = Decoder(num_tokens=num_tokens, d_model=d_model, max_len=max_len)\n",
        "\n",
        "        # Output projection layer\n",
        "        self.output_linear = nn.Linear(num_tokens, num_tokens)\n",
        "\n",
        "    def forward(self, src_tokens, tgt_tokens):\n",
        "        # Pass source tokens through encoder\n",
        "        encoder_output, encoder_hidden = self.encoder(src_tokens)\n",
        "        # Pass target tokens and encoder outputs through decoder\n",
        "        decoder_output = self.decoder(tgt_tokens, encoder_output, encoder_hidden)\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "    def generate(self, src_tokens, max_len=10, start_token=4): # 4 is\n",
        "        device = src_tokens.device\n",
        "\n",
        "\n",
        "        # Encode the source sequence\n",
        "        encoder_output, encoder_hidden = self.encoder(src_tokens)\n",
        "\n",
        "        # Initialize decoder input with start token\n",
        "        decoder_input = torch.tensor([word_map['']])#+[word_map['<pad>']] * (max_len - 1))\n",
        "        decoder_input = decoder_input.unsqueeze(0).to(device) #batch size\n",
        "\n",
        "\n",
        "        generated_sequence = ['']\n",
        "\n",
        "        # Generate tokens one by one\n",
        "        for iter in range(max_len):\n",
        "            #print(f\"Iteration {iter}\")\n",
        "            # Get decoder output\n",
        "            decoder_output = self.decoder(decoder_input, encoder_output, encoder_hidden)\n",
        "            decoder_output = decoder_output.squeeze(0)\n",
        "            print(decoder_output)\n",
        "\n",
        "\n",
        "            # Get the predicted token\n",
        "            _, topi = decoder_output[-1].topk(1)\n",
        "            predicted_token = topi.item()\n",
        "            predicted_token= torch.tensor(predicted_token).to(device)\n",
        "            predicted_token_word = key = next((k for k, v in word_map.items() if v == predicted_token.item()), None)\n",
        "            # Add to the generated sequence\n",
        "            generated_sequence.append(predicted_token_word)\n",
        "\n",
        "\n",
        "            # Stop if we generated an  token\n",
        "            if predicted_token == word_map['<end>']:\n",
        "                break\n",
        "\n",
        "            # Update decoder input\n",
        "            predicted_token = predicted_token.view(1, 1)\n",
        "            decoder_input = torch.cat([decoder_input, predicted_token], dim=1)\n",
        "\n",
        "        return generated_sequence\n"
      ],
      "metadata": {
        "id": "uRptHOJemrlG"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = Transformer(num_tokens=num_tokens, d_model=d_model, max_len=max_len)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "1jo2SK7bm3_g"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2zzTgKflXYM",
        "outputId": "86a7a47e-be8a-4c35-b730-5469f5a4cf1a"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (we): Embedding(18244, 2)\n",
              "    (pe): PositionEncoding()\n",
              "    (self_attention): Attention(\n",
              "      (W_q): Linear(in_features=2, out_features=2, bias=False)\n",
              "      (W_k): Linear(in_features=2, out_features=2, bias=False)\n",
              "      (W_v): Linear(in_features=2, out_features=2, bias=False)\n",
              "    )\n",
              "    (layernorm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
              "    (fc_layer): Linear(in_features=2, out_features=18244, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (we): Embedding(18244, 2)\n",
              "    (pe): PositionEncoding()\n",
              "    (self_attention): Attention(\n",
              "      (W_q): Linear(in_features=2, out_features=2, bias=False)\n",
              "      (W_k): Linear(in_features=2, out_features=2, bias=False)\n",
              "      (W_v): Linear(in_features=2, out_features=2, bias=False)\n",
              "    )\n",
              "    (cross_attention): Attention(\n",
              "      (W_q): Linear(in_features=2, out_features=2, bias=False)\n",
              "      (W_k): Linear(in_features=2, out_features=2, bias=False)\n",
              "      (W_v): Linear(in_features=2, out_features=2, bias=False)\n",
              "    )\n",
              "    (layernorm1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
              "    (layernorm2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
              "    (fc_layer): Linear(in_features=2, out_features=18244, bias=True)\n",
              "  )\n",
              "  (output_linear): Linear(in_features=18244, out_features=18244, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    training_loss=[]\n",
        "    validation_loss=[]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        print(f\"\\nüåü Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Training loop\n",
        "        for i, (src, tgt_in, tgt_out) in enumerate(train_loader):\n",
        "            try:\n",
        "                optimizer.zero_grad()\n",
        "                src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
        "\n",
        "                #print(f\"üîÅ Training Iteration {i}\")\n",
        "\n",
        "                output = model(src, tgt_in)\n",
        "                output_flat = output.contiguous().view(-1, num_tokens)\n",
        "                target_flat = tgt_out.contiguous().view(-1)\n",
        "\n",
        "                loss = criterion(output_flat, target_flat)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error at training iteration {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        training_loss.append(epoch_loss / len(train_loader))\n",
        "        print(f\"‚úÖ Epoch {epoch + 1} Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for j, (src, tgt_in, tgt_out) in enumerate(val_loader):\n",
        "                try:\n",
        "                    src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
        "                    output = model(src, tgt_in)\n",
        "                    output_flat = output.contiguous().view(-1, num_tokens)\n",
        "                    target_flat = tgt_out.contiguous().view(-1)\n",
        "\n",
        "                    loss = criterion(output_flat, target_flat)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error at validation iteration {j}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        validation_loss.append(avg_val_loss)\n",
        "        print(f\"üß™ Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        model.train()  # switch back to training mode\n",
        "    return training_loss, validation_loss\n"
      ],
      "metadata": {
        "id": "cs4Wx2IwpBlv"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "avg_train_loss, avg_val_loss=train()"
      ],
      "metadata": {
        "id": "IIhgMtZXqZ-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cbdb63-f662-442c-e1c3-e2d689d97cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üåü Epoch 1/200\n",
            "‚ùå Error at training iteration 4423: The size of tensor a (25) must match the size of tensor b (13) at non-singleton dimension 1\n",
            "‚úÖ Epoch 1 Training Loss: 3.5306\n",
            "‚ùå Error at validation iteration 1894: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 1\n",
            "üß™ Epoch 1 Validation Loss: 2.4741\n",
            "\n",
            "üåü Epoch 2/200\n",
            "‚ùå Error at training iteration 4423: The size of tensor a (25) must match the size of tensor b (13) at non-singleton dimension 1\n",
            "‚úÖ Epoch 2 Training Loss: 2.4493\n",
            "‚ùå Error at validation iteration 1894: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 1\n",
            "üß™ Epoch 2 Validation Loss: 2.4465\n",
            "\n",
            "üåü Epoch 3/200\n",
            "‚ùå Error at training iteration 4423: The size of tensor a (25) must match the size of tensor b (13) at non-singleton dimension 1\n",
            "‚úÖ Epoch 3 Training Loss: 2.4208\n",
            "‚ùå Error at validation iteration 1894: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 1\n",
            "üß™ Epoch 3 Validation Loss: 2.4221\n",
            "\n",
            "üåü Epoch 4/200\n",
            "‚ùå Error at training iteration 4423: The size of tensor a (25) must match the size of tensor b (13) at non-singleton dimension 1\n",
            "‚úÖ Epoch 4 Training Loss: 2.3995\n",
            "‚ùå Error at validation iteration 1894: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 1\n",
            "üß™ Epoch 4 Validation Loss: 2.4028\n",
            "\n",
            "üåü Epoch 5/200\n",
            "‚ùå Error at training iteration 4423: The size of tensor a (25) must match the size of tensor b (13) at non-singleton dimension 1\n",
            "‚úÖ Epoch 5 Training Loss: 2.3700\n",
            "‚ùå Error at validation iteration 1894: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 1\n",
            "üß™ Epoch 5 Validation Loss: 2.3750\n",
            "\n",
            "üåü Epoch 6/200\n",
            "‚ùå Error at training iteration 4423: The size of tensor a (25) must match the size of tensor b (13) at non-singleton dimension 1\n",
            "‚úÖ Epoch 6 Training Loss: 2.3326\n",
            "‚ùå Error at validation iteration 1894: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 1\n",
            "üß™ Epoch 6 Validation Loss: 2.3285\n",
            "\n",
            "üåü Epoch 7/200\n",
            "‚ùå Error at training iteration 4423: The size of tensor a (25) must match the size of tensor b (13) at non-singleton dimension 1\n",
            "‚úÖ Epoch 7 Training Loss: 2.2941\n",
            "‚ùå Error at validation iteration 1894: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 1\n",
            "üß™ Epoch 7 Validation Loss: 2.2938\n",
            "\n",
            "üåü Epoch 8/200\n",
            "‚ùå Error at training iteration 4423: The size of tensor a (25) must match the size of tensor b (13) at non-singleton dimension 1\n",
            "‚úÖ Epoch 8 Training Loss: 2.2649\n",
            "‚ùå Error at validation iteration 1894: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 1\n",
            "üß™ Epoch 8 Validation Loss: 2.2659\n",
            "\n",
            "üåü Epoch 9/200\n",
            "‚ùå Error at training iteration 4423: The size of tensor a (25) must match the size of tensor b (13) at non-singleton dimension 1\n",
            "‚úÖ Epoch 9 Training Loss: 2.2342\n",
            "‚ùå Error at validation iteration 1894: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 1\n",
            "üß™ Epoch 9 Validation Loss: 2.2391\n",
            "\n",
            "üåü Epoch 10/200\n",
            "‚ùå Error at training iteration 4423: The size of tensor a (25) must match the size of tensor b (13) at non-singleton dimension 1\n",
            "‚úÖ Epoch 10 Training Loss: 2.2141\n",
            "‚ùå Error at validation iteration 1894: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 1\n",
            "üß™ Epoch 10 Validation Loss: 2.2171\n",
            "\n",
            "üåü Epoch 11/200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"transformer.pth\")\n",
        "model.load_state_dict(torch.load(\"transformer.pth\"))"
      ],
      "metadata": {
        "id": "xq019pzytSwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_losses(train_losses, val_losses):\n",
        "    epochs = list(range(1, len(train_losses) + 1))\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(epochs, train_losses, label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4ouVGB4BVc7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_input = torch.tensor([[word_map['your'], word_map['test'], word_map['sequence']]], device=device)\n",
        "    output_tokens = model.generate(test_input, max_len=20, start_token=word_map['<start>'])\n",
        "    print(output_tokens)\n",
        "    print(\"Generated:\", ' '.join([tok for tok in output_tokens if tok not in ['<pad>', '<end>', '<start>', None]]))"
      ],
      "metadata": {
        "id": "Pneq9omSlsju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(avg_train_loss, avg_val_loss)"
      ],
      "metadata": {
        "id": "5f-PsP2oVkQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (src, tgt_in, tgt_out) in enumerate(test_loader):\n",
        "  src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
        "  response =model.generate(src, max_len=25, start_token=word_map[\"<start>\"])\n",
        "  sentence = []\n",
        "  for token in response:\n",
        "    if token == '<end>':\n",
        "        break\n",
        "    if token != '<pad>':\n",
        "        sentence.append(token)\n",
        "\n",
        "  # Join into sentence\n",
        "  print(sentence)\n",
        "  final_sentence = ' '.join(sentence).strip()\n",
        "  print(final_sentence)\n",
        "\n"
      ],
      "metadata": {
        "id": "wr2SJYjltdjD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}