{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tannisthamaiti/AIWeekend-Project/blob/main/NLP_WordEmbeddings_CNN/NLP_WordEmbeddings_CNN_Question_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed5330c1",
      "metadata": {
        "id": "ed5330c1"
      },
      "source": [
        "In this assignment, you will practice how to compute word embeddings and use them for sentiment analysis.\n",
        "\n",
        "To implement sentiment analysis, you can go beyond counting the number of positive words and negative words.\n",
        "You can find a way to represent each word numerically, by a vector.\n",
        "The vector could then represent syntactic (i.e. parts of speech) and semantic (i.e. meaning) structures.\n",
        "In this assignment, you will explore a classic way of generating word embeddings or representations.\n",
        "\n",
        "You will implement a famous model called the continuous bag of words (CBOW) model.\n",
        "By completing this assignment you will:\n",
        "\n",
        "\n",
        "\n",
        "*   Train word vectors from scratch\n",
        "*   Learn how to create batches of data.\n",
        "*   Understand how backpropagation works.\n",
        "*   Plot and visualize your learned word vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the following sentence:\n",
        ">**'I am happy because I am learning AI'**.\n",
        "\n",
        "- In continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).\n",
        "- For example, if you were to choose a context half-size of say $C = 2$, then you would try to predict the word **happy** given the context that includes 2 words before and 2 words after the center word:\n",
        "\n",
        "> $C$ words before: [I, am]\n",
        "\n",
        "> $C$ words after: [because, I]\n",
        "\n",
        "- In other words:\n",
        "\n",
        "$$context = [I,am, because, I]$$\n",
        "$$target = happy$$\n",
        "\n",
        "Once you have encoded all the context words, you can use $\\bar x$ as the input to your model.\n",
        "\n",
        "The architecture you will be implementing is as follows:\n",
        "\n",
        "\\begin{align}\n",
        " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
        " a &= ReLU(h)  \\tag{2} \\\\\n",
        " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
        " \\hat y &= softmax(z)   \\tag{4} \\\\\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "gYCD3JGI-lPw"
      },
      "id": "gYCD3JGI-lPw"
    },
    {
      "cell_type": "markdown",
      "id": "0d956ba9",
      "metadata": {
        "id": "0d956ba9"
      },
      "source": [
        "## Forward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b11906",
      "metadata": {
        "id": "60b11906"
      },
      "source": [
        "Let's dive into the neural network itself, which is shown below with all the dimensions and formulas you'll need.\n",
        "\n",
        "![CBOW Model](https://github.com/tannisthamaiti/AIWeekend-Project/blob/main/images/cbow_model_dimensions_single_input.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40919bc2",
      "metadata": {
        "id": "40919bc2"
      },
      "source": [
        "Set $N$ equal to 3. Remember that $N$ is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\n",
        "\n",
        "Also set $V$ equal to 5, which is the size of the vocabulary we have used so far."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Python libraries and helper functions (in utils2)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from utils2 import sigmoid, get_batches, compute_pca, get_dict"
      ],
      "metadata": {
        "id": "EX5zO3OXAIXQ"
      },
      "id": "EX5zO3OXAIXQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download sentence tokenizer\n",
        "nltk.data.path.append('.')"
      ],
      "metadata": {
        "id": "3eLzMmdFARhI"
      },
      "id": "3eLzMmdFARhI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1(a)"
      ],
      "metadata": {
        "id": "pm54atrFAZZS"
      },
      "id": "pm54atrFAZZS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, tokenize and process the data\n",
        "import re                                                           #  Load the Regex-modul\n",
        "with open('shakespeare.txt') as f:                                  # file location https://github.com/tannisthamaiti/AIWeekend-Project/blob/main/NLP_WordEmbeddings_CNN/shakespeare.txt\n",
        "    data = f.read()                                                 #  Read in the data\n",
        "data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
        "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
        "data = #Your code here            #  Lower case and drop non-alphabetical tokens (use isalpha, list comphrehensive)\n",
        "print(\"Number of tokens:\", len(data),'\\n', data[:15])              #  print data sample"
      ],
      "metadata": {
        "id": "jyld4uckAYPj"
      },
      "id": "jyld4uckAYPj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Output\n",
        "\n",
        "Number of tokens: 17395\n",
        "\n",
        " ['the', 'sonnets', 'by', 'william', 'shakespeare', 'from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that', 'thereby', 'beauty', 'rose']"
      ],
      "metadata": {
        "id": "7ZrveDGYEWBG"
      },
      "id": "7ZrveDGYEWBG"
    },
    {
      "cell_type": "code",
      "source": [
        "data = [word.lower() for word in data if word.isalpha()]\n",
        "print(\"Number of tokens:\", len(data),'\\n', data[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s_8fzIWDqoB",
        "outputId": "503014d3-8624-43e6-eb43-b40957f2fc3c"
      },
      "id": "7s_8fzIWDqoB",
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 17395 \n",
            " ['the', 'sonnets', 'by', 'william', 'shakespeare', 'from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that', 'thereby', 'beauty', 'rose']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
        "fdist = nltk.FreqDist(word for word in data)\n",
        "print(\"Size of vocabulary: \",len(fdist) )\n",
        "print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq.\n"
      ],
      "metadata": {
        "id": "WjVforKPA51-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46243083-70b6-4ced-b05b-35b6088b6930"
      },
      "id": "WjVforKPA51-",
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  3001\n",
            "Most frequent tokens:  [('and', 490), ('the', 432), ('to', 408), ('my', 393), ('of', 370), ('i', 349), ('that', 323), ('in', 323), ('thy', 287), ('thou', 234), ('love', 188), ('with', 181), ('is', 180), ('not', 176), ('for', 171), ('me', 164), ('but', 163), ('a', 163), ('thee', 162), ('so', 145)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mapping words to indices and indices to words\n",
        "We provide a helper function to create a dictionary that maps words to indices and indices to words."
      ],
      "metadata": {
        "id": "hndZ8dC-BMwk"
      },
      "id": "hndZ8dC-BMwk"
    },
    {
      "cell_type": "code",
      "source": [
        "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
        "word2Ind, Ind2word = # Your implementation\n",
        "V = len(word2Ind)\n",
        "print(\"Size of vocabulary: \", V)"
      ],
      "metadata": {
        "id": "IuQtAfvKBYDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01527c5-32d3-4359-b918-f09c1bbf8bb8"
      },
      "id": "IuQtAfvKBYDS",
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  3001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "Size of vocabulary:  3001"
      ],
      "metadata": {
        "id": "F2IYhGXeB2hK"
      },
      "id": "F2IYhGXeB2hK"
    },
    {
      "cell_type": "code",
      "source": [
        "# example of word to index mapping\n",
        "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
        "print(\"Index of the word 'king' :  \",word2Ind['queen'] )\n",
        "print(\"Word which has index 2743:  \",Ind2word[2743] )"
      ],
      "metadata": {
        "id": "nHl6YXElCGWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040b9d7b-dcd9-438f-fb67-7c16b603f2fb"
      },
      "id": "nHl6YXElCGWK",
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of the word 'king' :   1404\n",
            "Index of the word 'king' :   1983\n",
            "Word which has index 2743:   upon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "Index of the word 'king' :   1404\n",
        "\n",
        "Index of the word 'king' :   1983\n",
        "\n",
        "Word which has index 2743:   upon"
      ],
      "metadata": {
        "id": "m6pqKVs1CO_m"
      },
      "id": "m6pqKVs1CO_m"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}