{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9f40c232-df6e-49df-9016-6459e4af2e1e",
      "metadata": {
        "tags": [],
        "id": "9f40c232-df6e-49df-9016-6459e4af2e1e"
      },
      "source": [
        "# Coding Self-Attention in PyTroch!!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4226d63-8d76-40bc-a8e6-0f290a159418",
      "metadata": {
        "id": "f4226d63-8d76-40bc-a8e6-0f290a159418"
      },
      "source": [
        "In this tutorial, you will...\n",
        "\n",
        "- **[Code a Basic Self-Attention Class!!!](#selfAttention)** The basic self-attention class allows the transformer to establish relationships among words and tokens.\n",
        "\n",
        "- **[Calculate Self-Attention Values!!!](#calculate)** We'll then use the class that we created, SelfAttention, to calculate self-attention values for some sample data.\n",
        "\n",
        "- **[Verify The Calculations!!!](#validate)** Lastly, we'll validate the calculations made by the SelfAttention class..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "855d3a52-a43e-44cf-b8b6-b2ce88bea382",
      "metadata": {
        "id": "855d3a52-a43e-44cf-b8b6-b2ce88bea382"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63b86036-1369-441c-a14d-3ba7bdaa103b",
      "metadata": {
        "tags": [],
        "id": "63b86036-1369-441c-a14d-3ba7bdaa103b"
      },
      "source": [
        "# Import the modules that will do all the work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5c520e0b-c6e4-43ce-93f5-c0f2b5e75438",
      "metadata": {
        "height": 79,
        "id": "5c520e0b-c6e4-43ce-93f5-c0f2b5e75438"
      },
      "outputs": [],
      "source": [
        "import torch ## torch let's us create tensors and also provides helper functions\n",
        "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
        "import torch.nn.functional as F # This gives us the softmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b01434-9796-4f5f-9d39-341505c912d6",
      "metadata": {
        "id": "56b01434-9796-4f5f-9d39-341505c912d6"
      },
      "source": [
        "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> ðŸ’» &nbsp; <b>Access <code>requirements.txt</code> file:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e58ebb2-1798-41c3-9ff8-1b4f50605964",
      "metadata": {
        "id": "6e58ebb2-1798-41c3-9ff8-1b4f50605964"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "125c4f0d-83b6-488f-8d91-66c54776bf2f",
      "metadata": {
        "id": "125c4f0d-83b6-488f-8d91-66c54776bf2f"
      },
      "source": [
        "# Code Self-Attention\n",
        "<a id=\"selfAttention\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e3392130-cd25-4000-97bb-9612764c83a8",
      "metadata": {
        "height": 878,
        "id": "e3392130-cd25-4000-97bb-9612764c83a8"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1):\n",
        "        ## d_model = the number of embedding values per token.\n",
        "        ##           Because we want to be able to do the math by hand, we've\n",
        "        ##           the default value for d_model=2.\n",
        "        ##           However, in \"Attention Is All You Need\" d_model=512\n",
        "        ##\n",
        "        ## row_dim, col_dim = the indices we should use to access rows or columns\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        ## Initialize the Weights (W) that we'll use to create the\n",
        "        ## query (q), key (k) and value (v) for each token\n",
        "        ## NOTE: A lot of implementations include bias terms when\n",
        "        ##       creating the the queries, keys, and values, but\n",
        "        ##       the original manuscript that described Attention,\n",
        "        ##       \"Attention Is All You Need\" did not, so we won't either\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "        self.row_dim = row_dim\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "\n",
        "    def forward(self, token_encodings):\n",
        "        ## Create the query, key and values using the encoding numbers\n",
        "        ## associated with each token (token encodings)\n",
        "        q = self.W_q(token_encodings)\n",
        "        k = self.W_k(token_encodings)\n",
        "        v = self.W_v(token_encodings)\n",
        "\n",
        "        ## Compute similarities scores: (q * k^T)\n",
        "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
        "\n",
        "        ## Scale the similarities by dividing by sqrt(k.col_dim)\n",
        "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
        "\n",
        "        ## Apply softmax to determine what percent of each tokens' value to\n",
        "        ## use in the final attention values.\n",
        "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
        "\n",
        "        ## Scale the values by their associated percentages and add them up.\n",
        "        attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "        return attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371cd1e0-0216-438d-a241-05533e4b374d",
      "metadata": {
        "id": "371cd1e0-0216-438d-a241-05533e4b374d"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c75aaaa3-29e0-4c03-8a11-816bd4b7aea3",
      "metadata": {
        "id": "c75aaaa3-29e0-4c03-8a11-816bd4b7aea3"
      },
      "source": [
        "# Calculate Self-Attention\n",
        "<a id=\"calculate\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6c17a35a-ceee-4502-9a25-9adbe6a0a1a8",
      "metadata": {
        "height": 283,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c17a35a-ceee-4502-9a25-9adbe6a0a1a8",
        "outputId": "ea2e578d-279a-42f4-8017-ae70e8853c99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0100, 1.0641],\n",
              "        [0.2040, 0.7057],\n",
              "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "## create a matrix of token encodings...\n",
        "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
        "                                 [0.57, 1.36],\n",
        "                                 [4.41, -2.16]])\n",
        "\n",
        "## set the seed for the random number generator\n",
        "torch.manual_seed(42)\n",
        "\n",
        "## create a basic self-attention ojbect\n",
        "selfAttention = SelfAttention(d_model=2,\n",
        "                               row_dim=0,\n",
        "                               col_dim=1)\n",
        "\n",
        "## calculate basic attention for the token encodings\n",
        "selfAttention(encodings_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1153eec7-5573-41f7-b7de-3106b2ca07f9",
      "metadata": {
        "id": "1153eec7-5573-41f7-b7de-3106b2ca07f9"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c25086ad-92d0-4396-8484-d9dab376ef50",
      "metadata": {
        "id": "c25086ad-92d0-4396-8484-d9dab376ef50"
      },
      "source": [
        "# Print Out Weights and Verify Calculations\n",
        "<a id=\"validate\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0ceddcba-be60-46fe-80bd-fa74e97ea62f",
      "metadata": {
        "height": 62,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ceddcba-be60-46fe-80bd-fa74e97ea62f",
        "outputId": "58e8cc72-7e8c-4cdc-91c1-4f22e8181c22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5406, -0.1657],\n",
              "        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "## print out the weight matrix that creates the queries\n",
        "selfAttention.W_q.weight.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "03331fba-9985-48e0-92ba-d72118c7a9c0",
      "metadata": {
        "height": 62,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03331fba-9985-48e0-92ba-d72118c7a9c0",
        "outputId": "3066cb34-3339-4c77-97c2-878619cff407"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1549, -0.3443],\n",
              "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "## print out the weight matrix that creates the keys\n",
        "selfAttention.W_k.weight.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "89d1e184-4fd0-4e6c-bd24-6916df2f76d8",
      "metadata": {
        "height": 62,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89d1e184-4fd0-4e6c-bd24-6916df2f76d8",
        "outputId": "b6ab6cd8-afab-4b78-908e-c12c6780d615"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6233,  0.6146],\n",
              "        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "## print out the weight matrix that creates the values\n",
        "selfAttention.W_v.weight.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "55d27388-5b15-4ee5-b39c-61903df18286",
      "metadata": {
        "height": 47,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55d27388-5b15-4ee5-b39c-61903df18286",
        "outputId": "9c98351f-0eed-404d-fe85-24044a410bb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7621, -0.0428],\n",
              "        [ 1.1063,  0.7890],\n",
              "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "## calculate the queries\n",
        "selfAttention.W_q(encodings_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c4b3425e-7b2f-4282-b4df-a2da6712c1c6",
      "metadata": {
        "height": 47,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4b3425e-7b2f-4282-b4df-a2da6712c1c6",
        "outputId": "c92a7c03-fa36-466c-e083-dead0a7971c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1469, -0.3038],\n",
              "        [ 0.1057,  0.3685],\n",
              "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "## calculate the keys\n",
        "selfAttention.W_k(encodings_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9ff03c5b-62a6-4cda-94fc-1aac994e4ad5",
      "metadata": {
        "height": 47,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ff03c5b-62a6-4cda-94fc-1aac994e4ad5",
        "outputId": "6785cb18-2155-4b3f-edce-c45a1fbec9f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6038,  0.7434],\n",
              "        [-0.3502,  0.5303],\n",
              "        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "## calculate the values\n",
        "selfAttention.W_v(encodings_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7d88e411-84ac-4759-abcf-512cf8ff0670",
      "metadata": {
        "height": 47,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d88e411-84ac-4759-abcf-512cf8ff0670",
        "outputId": "7c5ec980-830d-4fe1-b607-cdb6e7e4f6d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7621, -0.0428],\n",
              "        [ 1.1063,  0.7890],\n",
              "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "q = selfAttention.W_q(encodings_matrix)\n",
        "q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "95e191e2-fab3-4290-8085-b64b436dc5e7",
      "metadata": {
        "height": 47,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95e191e2-fab3-4290-8085-b64b436dc5e7",
        "outputId": "cc625707-a9f8-4c7d-c192-180e011e7088"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1469, -0.3038],\n",
              "        [ 0.1057,  0.3685],\n",
              "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "k = selfAttention.W_k(encodings_matrix)\n",
        "k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b4f55998-e89b-40b6-9021-f764ba32a3b5",
      "metadata": {
        "height": 62,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4f55998-e89b-40b6-9021-f764ba32a3b5",
        "outputId": "b1e34b69-529f-43e1-f1bb-82f86ef0e0c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0990,  0.0648, -0.6523],\n",
              "        [-0.4022,  0.4078, -3.0024],\n",
              "        [ 0.4842, -0.6683,  4.0461]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "sims = torch.matmul(q, k.transpose(dim0=0, dim1=1))\n",
        "sims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "84693537-1874-4698-963a-75c9075b4bb7",
      "metadata": {
        "height": 62,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84693537-1874-4698-963a-75c9075b4bb7",
        "outputId": "4524e523-20fe-4494-95e8-ab529741ddd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0700,  0.0458, -0.4612],\n",
              "        [-0.2844,  0.2883, -2.1230],\n",
              "        [ 0.3424, -0.4725,  2.8610]], grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "scaled_sims = sims / (torch.tensor(2)**0.5)\n",
        "scaled_sims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9bbc718e-c73c-4713-8b29-73962e39645d",
      "metadata": {
        "height": 62,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bbc718e-c73c-4713-8b29-73962e39645d",
        "outputId": "5c027829-e368-4b65-a552-bf486fdf55ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3573, 0.4011, 0.2416],\n",
              "        [0.3410, 0.6047, 0.0542],\n",
              "        [0.0722, 0.0320, 0.8959]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "attention_percents = F.softmax(scaled_sims, dim=1)\n",
        "attention_percents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8c2edce0-9720-41ba-8916-e3415ef05e81",
      "metadata": {
        "height": 45,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c2edce0-9720-41ba-8916-e3415ef05e81",
        "outputId": "383974d8-5c0e-4f0d-c8ce-efbb0eaac6b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0100, 1.0641],\n",
              "        [0.2040, 0.7057],\n",
              "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "torch.matmul(attention_percents, selfAttention.W_v(encodings_matrix))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}